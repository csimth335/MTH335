
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"> 
    <meta name="viewport" content="width=device-width, initial-scale=1">




<link
  href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css"
  rel="stylesheet">

<style>
.julia {font-family: "Source Code Pro";
        color:#0033CC;
        }
body { padding-top: 60px; }
h5:before {content:"\2746\ ";}
h6:before {content:"\2742\ ";}
pre {display: block;}
</style>

<script src="http://code.jquery.com/jquery.js"></script>
<script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>

<!-- .julia:before {content: "julia> "} -->

<style></style>



<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ["\$","\$"], ["\\(","\\)"]]
  },
  displayAlign: "left",
  displayIndent: "5%"
});
</script>

<script type="text/javascript">
$( document ).ready(function() {
  $("h1").each(function(index) { 
       var title = $( this ).text()
       $("#page_title").html("<strong>" + title + "</strong>");
       document.title = title
  });
  $( "h2" ).each(function( index ) {
    var nm =  $( this ).text();                                    
    var id = $.trim(nm).replace(/ /g,'');
    this.id = id
    $("#page_dropdown").append("<li><a href='#" + id + "'>" + nm + "</a></li>");
  });
  $('[data-toggle="popover"]').popover();  
});
</script>

</head>


<body data-spy="scroll" >

<nav class="navbar navbar-default  navbar-fixed-top">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
         
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li><a href="#" id="page_title"></a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
         <li class="dropdown">
           <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
           Jump to... <span class="caret"></span></a>
          <ul class="dropdown-menu" role="menu" id="page_dropdown"></ul>
        </li>
      </ul>
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

<header>
</header>

<div class="container-fluid">
  <div class="span10 offset1">
<h1>Review for test 2</h1><p>This is some of the material that will be up for testing on test 2.</p><h2>Chapter 3</h2><p>In Chapter 3 we discussed various means to solve the equation &#36;f&#40;x&#41;&#61;0&#36; where &#36;f: R \rightarrow R&#36;. (That is a &#36;f&#36; is a scalar-valued function of a real variable). This formulation applies to finding answers for each of these equations:</p>$$~
\begin{align}
\sin(x) -x + 1/2 &= 0\\
\tan(x) + x &= 1\\
a\cos(x) &= x
\end{align}
~$$<p>Here is a summary of the methods we discussed:</p><h3>Bisection method:</h3><ul>
<li>assumes &#36;f&#40;x&#41;&#36; is &#36;C&#36; (continuous).</li>
</ul><ul>
<li>&#36;c_i &#61; mid&#40;a_i, b_i&#41;&#36;; and &#36;&#91;a_&#123;i&#43;1&#125;, b_&#123;i&#43;1&#125;&#93;&#36; chosen from &#36;&#91;a_i, c_i&#93;&#36; or &#36;&#91;c_i, b_i&#93;&#36;.</li>
</ul><ul>
<li>The error, &#36;e_n &#61; c_n -r&#36;  is bounded by &#36;2^&#123;-&#40;n&#43;1&#41;&#125;&#40;b_0 - a_0&#41;&#36;.</li>
</ul><ul>
<li>guaranteed (mathematically) to converge (&#36;c_n \rightarrow r&#36;) if &#36;a_0, b_0&#36; brackets a root. (Intermediate value theorem.)</li>
</ul><ul>
<li>linear convergence</li>
</ul><ul>
<li>in floating point can be guaranteed to produce a floating point number &#36;r&#36; where it may not be &#36;f&#40;r&#41;&#61;0&#36;, but if not, the `f(prevfloat(r)) * f(nextfloat(r)) < 0$.</li>
</ul><h3>Newton's method</h3><ul>
<li>assumes &#36;f&#40;x&#41;&#36; is &#36;C^2&#36; and &#36;r&#36; is a <em>simple</em> zero</li>
</ul><ul>
<li>a process &#36;x_&#123;n&#43;1&#125; &#61; x_n - f&#40;x_n&#41; / f&#39;&#40;x_n&#41;&#36;</li>
</ul><ul>
<li>the error satisfies &#36;e_&#123;n&#43;1&#125; &#61; f&#39;&#39;&#40;\xi&#41;/&#40;2f&#39;&#40;x_n&#41;&#41; e_n^2 \approx f&#39;&#39;&#40;r&#41;/&#40;2f&#39;&#40;r&#41;&#41; e_n^2&#36;.</li>
</ul><ul>
<li>Let &#36;C&#40;\delta&#41; &#61; max&#40;f&#39;&#39;&#40;y&#41;&#41;/&#40;2min&#40;f&#39;&#40;y&#41;&#41;&#41;&#36; for &#36;x,y&#36; within &#36;\delta&#36; of &#36;r&#36;. If &#36;\delta C&#40;\delta&#41; &lt; 1&#36;, then any starting value within &#36;\delta&#36; of &#36;r&#36; will converge to &#36;r&#36; quadratically fast.</li>
</ul><ul>
<li>If &#36;f&#39;&gt;0&#36; and &#36;f&#39;&#39;&gt;0&#36; then any initial value will converge</li>
</ul><ul>
<li>numerically one needs to clarify when we stop iterating using different tolerances.</li>
</ul><ul>
<li>If &#36;r&#36; is not a simple zero, convergence is linear.</li>
</ul><h3>Secant method</h3><ul>
<li>Assume &#36;f&#36; is &#36;C^2&#36; and &#36;r&#36; is a simple zero.</li>
</ul><ul>
<li>if &#36;f&#91;a,b&#93; &#61; &#40;f&#40;b&#41; - f&#40;a&#41;&#41;/&#40;b-a&#41; \approx f&#39;&#40;a&#41;&#36;, then the method is &#36;x_&#123;n&#43;1&#125; &#61; x_n - f&#40;x_n&#41; / f&#91;x_n, x_&#123;n-1&#125;&#93;&#36;.</li>
</ul><ul>
<li>&#36;e_&#123;n&#43;1&#125; \approx f&#39;&#39;&#40;r&#41; / &#40;2f&#39;&#40;r&#41;&#41; e_n e_&#123;n-1&#125;&#36;.</li>
</ul><ul>
<li>convergence is faster than linear, slower than quadratic: &#36;\alpha &#61; &#40;1 &#43; \sqrt&#123;5&#125;&#41;/2&#36;</li>
</ul><h3>Fixed point</h3><ul>
<li>Assume &#36;F&#36; is &#36;C^q&#36; and &#36;r&#36; is a simple zero</li>
</ul><ul>
<li>method is &#36;x_&#123;n&#43;1&#125; &#61; F&#40;x_n&#41;&#36;.</li>
</ul><ul>
<li>If &#36;F&#36; is a <strong>contractive mapping</strong>, then the method converges to a (uinque) fixed point &#36;s&#36;.</li>
</ul><ul>
<li>If &#36;x_&#123;n&#43;1&#125; &#61; F&#40;x_n&#41;&#36; converges to &#36;s&#36; and If &#36;q&#36; is the first integer where &#36;F^&#123;&#40;q&#41;&#125;&#41;&#40;r&#41; \neq 0&#36;, then</li>
</ul><ul>
<li>&#36;e_&#123;n&#43;1&#125; &#61; F^&#123;&#40;q&#41;&#125;&#40;\xi&#41;/q&#33; e_n^q&#36;</li>
</ul><ul>
<li>&#36;q&#36; is the order of convergence.</li>
</ul><h3>some sample problems</h3><ul>
<li>Will the bisection method find the zero of &#36;&#40;x-1&#41;^2&#36;?</li>
</ul><ul>
<li>Will Newton's method find the zero of &#36;&#40;x-1&#41;^2&#36;?</li>
</ul><ul>
<li>In what ways does the function &#36;f&#40;x&#41; &#61; x^&#123;20&#125; - 1&#36; challenge the convergence of Newton's method?</li>
</ul><ul>
<li>Why do you know that using Newton's method on the function &#36;f&#40;x&#41; &#61; e^x &#43; x&#36; will be guaranteed to converge?</li>
</ul><ul>
<li>Let &#36;f&#40;x&#41; &#61; \sin&#40;x&#41;&#36;. Starting with the &#36;x_0, x_1 &#61; 3, 4&#36;, perform 2 steps of the secant method (find &#36;x_2, x_3&#36;). How accurate is your value (&#36;f&#40;x_3&#41; - \pi&#36;)?</li>
</ul><ul>
<li>The <em>efficiency index</em> is defined by &#36;\mu^&#123;1/f&#125;&#36; where &#36;\mu&#36; is the order of convergence of a method and &#36;f&#36; is the number of function calls per step. For Newton, these values are &#36;2&#36; and &#36;2&#36; and for the secant method &#36;&#40;1 &#43; \sqrt&#123;5&#125;&#41;/2&#36; and &#36;1&#36;. Which method has a bigger efficiency index?</li>
</ul><ul>
<li>Let &#36;e_n &#61; x_n -r&#36;, Taylor's theorem gives</li>
</ul>$$~
f(x_n) = f(r) + f'(r)e_n + (1/2) f''(r) e_n^2 + \mathcal{O}(e_n^3)
~$$<p>Divide this by &#36;e_n&#36;, then use for both the case &#36;n&#36; and &#36;n-1&#36;. Use these pieces to identify the limit as &#36;n&#36; goes to &#36;\infty&#36; of</p>$$~
\frac{f(x_n)/e_n - f(x_{n-1})/e_{n-1}}{x_n - x_{n-1}}.
~$$<ul>
<li>Let &#36;F&#40;x&#41; &#61; x/2 &#43; f&#40;x&#41;&#36; where we assume &#36;|f&#39;&#40;x&#41;| \leq 1/4&#36;. Show that &#36;F&#40;x&#41;&#36; is a contractive map.</li>
</ul><ul>
<li>Let &#36;F&#40;x&#41; &#61; x/2 &#43; f&#40;x&#41;&#36; where we assume &#36;|f&#39;&#40;x&#41;| \leq 1/4&#36;. What is the order of convergence of the iterative mapping: &#36;x_&#123;n&#43;1&#125; &#61; F&#40;x_n&#41;&#36;?</li>
</ul><ul>
<li>Let &#36;p&gt;0&#36;. What is the value of this expression:</li>
</ul>$$~
\sqrt{p + \sqrt{p + \sqrt{p + \cdots}}}
~$$<p>(As the book says, this is &#36;x_&#123;n&#43;1&#125; &#61; \sqrt&#123;p &#43; x_n&#125;&#36;.)</p><h2>Chapter 4</h2><p>Chapter 4 deals with the problem of simulataneous solutions of linear equations expressed in matrix form: &#36;Ax&#61;b&#36;.</p><h3>4.1</h3><p>In the first section, there is a review of linear algebra concepts. Some important pieces are:</p><ul>
<li>how &#36;Ax&#61;b&#36; represents a system of equations</li>
<li>the language of matrix, vector, column vector, row vector</li>
<li>matrix operations: scalar multiplication, addition, subtraction, multiplication, transpose.</li>
<li>symmetric, positive definite matrices.</li>
<li>block multiplication of matrices</li>
<li>the inverse of a matrix</li>
<li>special types of matrices: square, invertible, symmetric, upper triangular, lower triangular, etc.</li>
</ul><h3>4.2</h3><p>We looked at solving &#36;Ax&#61;b&#36; is special cases: diagonal matrices, upper triangular, or lower triangular.</p><p>We looked at forming an LU decomposition of a matrix &#36;A&#36;. The first step of the method basically forms this matrix product:</p>$$~
A = \left[
\begin{array}{cc}
a_{11} & c^T\\
b & A_1
\end{array}
\right]
= L_1 \cdot U_1
= \left[
\begin{array}{cc}
1 & 0\\
b/a_{11} & L_2
\end{array}
\right]
\cdot
\left[
\begin{array}{cc}
a_{11} & c^t\\
0 & R_2
\end{array}
\right]
~$$<p>(We won't do this method though, but you should know how to do Gaussian Elimination to find &#36;LU&#36;, with a possible &#36;P&#36;.)</p><p>We saw a block-matrix multiplication method to find the Cholesky decomposition. It starts with</p>$$~
A = \left[
\begin{array}{cc}
a_{11} & w^T\\
w & K
\end{array}
\right],
\quad
L_1 = \left[
\begin{array}{cc}
a_{11} & w^T\\
w & K
\end{array}
\right]
\quad\text{ and }

B_1= \left[
\begin{array}{cc}
I & 0^T\\
0 & K - \frac{ww^T}{a_{11}}
\end{array}
\right].
~$$<p>And then we have &#36;A &#61; L_1 B_1 L_1^T&#36;, and we can repeat the work on &#36;B_1&#36; to get a decomposition.</p><p>We had these theorems:</p><blockquote>
<p>If all &#36;n&#36; leading principal minors of &#36;A&#36; are non-singular, then &#36;A&#36; has an &#36;LU&#36; decomposition (without pivoting).</p>
</blockquote><blockquote>
<p>If &#36;A&#36; is real, symmetric and positive definite then it has a unique (Cholesky) factorization &#36;A&#61;LL^T&#36; and &#36;L&#36; has a positive diagonal.</p>
</blockquote><h2>4.3 Gaussian Elimination</h2><p>The Gaussian Elimination process produces a matrix &#36;U&#36;, possibly with row swaps, and if one counts the row multipliers appropriately, a matrix &#36;L&#36;, where &#36;LU&#61;PA&#36;, &#36;P&#36; being a permutation matrix.</p><p>The decomposition is not unique. In addition to having degrees of freedom to pick the diagonal values of &#36;L&#36; and &#36;U&#36;, it depends on  the choice of pivot row. We discussed briefly three pivot types:</p><ul>
<li>partial pivot</li>
<li>scaled pivot</li>
<li>complete pivoting</li>
</ul><p>you should know the partial pivoting, where the largest magnitude entry of &#36;a_&#123;li&#125;&#36;, in column &#36;i&#36;, after removing the previously chosen &#36;i-1&#36; rows is taken to be the pivot row. This choice ensures the values &#36;\lambda_i&#36; are all bounded by 1 in magnitude, so &#36;L&#36; will be unit triangular with dominant diagonal.</p><p>One compelling reason to choose to solve &#36;Ax&#61;b&#36; via solving &#36;LUx&#61;b&#36; is that it takes fewer steps than solving &#36;x &#61; A^&#123;-1&#125;b&#36;. The following theorem allows us to count:</p><blockquote>
<p>Theorem 4 (p176) on Long Operations: To solve &#36;Ax&#61;b&#36; for &#36;m&#36; vectors &#36;b&#36; where &#36;A&#36; is &#36;n\times n&#36; involves approximately this many <code>ops</code>:</p>
</blockquote>$$~
\frac{n^3}{3} + (\frac{1}{2} + m) n^2.
~$$<h2>Section 4.4</h2><p>Section 4.4 introduces the concept of a norm. This makes talking about convergence possible, as it gives  a sense of size.</p><p>We saw that a norm on a vector space was a function satisfying three properties:</p><ul>
<li>&#36;\| x \| &#61; 0&#36; implies &#36;x&#61;0&#36;;</li>
<li>&#36;\| \lambda x \| &#61; |\lambda| \cdot \| x \|&#36;;</li>
<li>&#36;\| x &#43; y \| \leq \|x \| &#43; \| y \|&#36;.</li>
</ul><p>There main vector norms a indexed: &#36;l_1&#36;, &#36;l_2&#36;, and &#36;l_\infty&#36;. For &#36;p \geq 1&#36;, we have also</p>$$~
\| x \|_p = (|x_1|^p +|x_2|^p +|x_3|^p +\cdots + |x_n|^p)^{1/p}.
~$$<p>The induced matrix norm is a norm on the set of matrices that is derived from a norm on vectors. It is specified by:</p>$$~
\| A \|_p = \text{The largest value of } \{\|Ax\|_p: \|x \|_p = 1\}.
~$$<p>We commented that when &#36;p&#61;2&#36;, this is related to the singular values of &#36;A&#36;, which are the eigen values of &#36;A^TA&#36;.</p><p>When &#36;p&#61;\infty&#36;, the &#36;\| \cdot \|_&#123;\infty&#125;&#36; norm is related to largest of the &#36;l_1&#36; norms of the column vectors of &#36;A&#36;.</p><p>A useful inequality of matrix norms is &#36;\| A B \| \leq \| A \| \cdot \| B \|&#36;, Similarly, &#36;\|Ax\| \leq \| A \| \cdot \| x \|&#36;. These are true for the matrix norm induced by the vector norm.</p><p>The use of norms make it possible to prove this inequality on the relative errors of solving &#36;Ax&#61;b&#36; when there is an error in finding &#36;b&#36;:</p>$$~
\frac{\| x - \tilde{x} \|}{\|x\|} \leq \kappa(A) \cdot \frac{\| b - \tilde{b} \|}{\|b\|}.
~$$<p>The condition number is &#36;\kappa&#40;A&#41; &#61; \| A \| \cdot \|A^&#123;-1&#125;\|&#36;. </p><h2>Iterative schemes</h2><p>We spoke about a few iterative schemes. The first is used to "polish" approximate answers. It goes like:</p><p>Suppose we have an approximate answer &#36;x^&#123;&#40;0&#41;&#125;&#36; to &#36;Ax&#61;b&#36;. Then we get the residual is</p>$$~
r^{(0)} = b - A x^{(0)}
~$$<p>And from here we solve the error from &#36;A e^&#123;&#40;0&#41;&#125; &#61; r^&#123;&#40;0&#41;&#125;&#36;. And finally, we get</p>$$~
x^{(1)} = x^{(0)} + e^{(0)}.
~$$<p>I everything is exact, then this is the answer, but if there is some loss in solving, then it is an improved answer. In that case the process can be repeated.</p><p>This can be made concrete by assuming we have an approximate inverse to &#36;A&#36;, called &#36;B&#36;. Then when solving &#36;Ae^&#123;&#40;0&#41;&#125; &#61; r^&#123;&#40;0&#125;&#41;&#36; by multiplying by &#36;B&#36;, we lose some information. In that case, the value of &#36;x^&#123;&#40;1&#41;&#125;&#36; satisfies:</p>$$~
r^{(0)} = b - A x^{(0)}; \quad e^{(0)} = B r^{(0)}; \quad x^{(1)} =x^{(0)} + e^{(0)}
~$$<p>And in general, the value of &#36;x^&#123;&#40;m&#41;&#125;&#36; satisfies:</p>$$~
x^{(m)} = B \sum_{k=0}^m (I - AB)^k b.
~$$<p>The sum on the right is convergent if &#36;\|I - AB \| &lt; 1&#36; and so &#36;x^&#123;&#40;m&#41;&#125;&#36; will converge.</p><p>This requires knowing that when &#36;\|A\|&lt;1&#36;, then &#36;I-A&#36; is invertible and can be written as a Neumann sum.</p><h3>Other convergent algorithms.</h3><p>If we have a <em>splitting matrix</em> &#36;Q&#36;, then we can form the equation &#36;Qx &#61; Qx - Ax &#43; b&#36;. Which suggests an iterative scheme of the type:</p>$$~
Q x^{(k+1)} = (Q- A)x^{(k)} + b.
~$$<p>For various choices of &#36;Q&#36; this will converge:</p><ul>
<li>&#36;Q&#61;I&#36; – Richardson method</li>
<li>&#36;Q&#61;diag&#40;A&#41;&#36; – Jacobi iteration</li>
<li>&#36;Q &#61; tril&#40;A&#41;&#36; – Gauss-Seidel.</li>
</ul><p>As long as &#36;\| I - Q^&#123;-1&#125;A \| &lt; 1&#36; for some subordinate nor, this sequence will converge.</p><p>More generally, if the iterative scheme is</p>$$~
x^{(k+1)} = Gx^{(k)} + c
~$$<p>then this will converge if the magnitude of the largest eigenvalue is less than 1.</p><h3>Some sample problems</h3><ul>
<li>Show that the product of lower triangular matrices is lower triangular using the characacteriaztion that &#36;L&#36; is LT if &#36;l_&#123;ij&#125;&#61;0&#36; when &#36;j &gt; i&#36; (the column index is bigger than the row one).</li>
</ul><ul>
<li>Solve for &#36;x&#36; in &#36;Ax&#61;b&#36;, when</li>
</ul>$$~
A = \left[
\begin{array}{ccc}
1 & 0 & 0\\
2 & 3 & 0\\
4 & 5 & 6
\end{array}
\right],
\quad
b = [7,8,9]^T
~$$<ul>
<li>Decompose &#36;A&#36; as a product of &#36;L\cdot U&#36; using Gaussian Elimination when &#36;A&#36; is</li>
</ul>$$~
A = \left[
\begin{array}{ccc}
2 & 0 & -8\\
-4 & -1 & 0\\
7 & 4 & -5
\end{array}
\right].
~$$<p>Did you need to use a permutation matrix?</p><p>Use your answer to solve &#36;Ax&#61;&#91;1,2,3&#93;^T&#36;.</p><ul>
<li>Find &#36;\| A\|_\infty&#36; when &#36;A&#36; is the matrix</li>
</ul>$$~
A = \left[
\begin{array}{ccc}
2 & 0 & -8\\
-4 & -1 & 0\\
7 & 4 & -5
\end{array}
\right].
~$$<ul>
<li>Find the condition number of &#36;A&#36; when &#36;A&#36; is the matrix:</li>
</ul>$$~
A = \left[
\begin{array}{ccc}
1 & 0 & 0\\
2 & 3 & 0\\
4 & 5 & 6
\end{array}
\right].
~$$
  </div>
</div>  

</body>
</html>
