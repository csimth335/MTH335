{
  "cells": [
     {"cell_type":"markdown","source":"<h1>Neumann series and iterative methods</h1>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>When we have a norm, &#36;\\| \\cdot \\|&#36;, we can talk about <em>convergence</em> of a sequence of vectors, &#36;v^k&#36; to a vector &#36;v&#36; or convergence of matrices.</p>","metadata":{}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Fact: For a finite dimensional vector space if &#36;v^k&#36; converges to &#36;v&#36; with one norm, it will with another.</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<p>Which is not the case on infinite dimensional spaces such as the space of functions.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>Example, iterations of a matrix &#36;A&#36;</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Take &#36;A&#36; to be a square matrix. Then we can form a series of matrices by</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nA^0, A^1, A^2, \\dots, A^n, \\dots\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>Example:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["2x2 Array{Float64,2}:\n 0.25  0.5 \n 0.5   0.25"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["A = (1/4) * [1 2; 2 1]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Then we have</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["(\n2x2 Array{Float64,2}:\n 0.25  0.5 \n 0.5   0.25,\n\n2x2 Array{Float64,2}:\n 0.3125  0.25  \n 0.25    0.3125,\n\n2x2 Array{Float64,2}:\n 0.203125  0.21875 \n 0.21875   0.203125,\n\n2x2 Array{Float64,2}:\n 0.160156  0.15625 \n 0.15625   0.160156)"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["A^1, A^2, A^3, A^4"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>The terms seem to be getting smaller.</p>","metadata":{}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Claim: &#36;A^k \\rightarrow 0&#36;.</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<p>Let&#39;s show the following: for a unit vector, we have &#36;&#40;4/3&#41;^k A^k u \\rightarrow &#40;1/2&#41; &#91;1,1&#93;&#36;.</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["2-element Array{Float64,1}:\n 0.333333\n 0.666667"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["n = 1; (4/3)^n * A^n * [1,0]"],"metadata":{},"execution_count":null},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["2-element Array{Float64,1}:\n 0.497942\n 0.502058"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["n =5; (4/3)^n * A^n * [1,0]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>And jumping ahead:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["2-element Array{Float64,1}:\n 0.5\n 0.5"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["n = 20; (4/3)^n * A^n * [1,0]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<blockquote>\n<p>Claim: &#36;\\sum A_k&#36; exists.</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<p>Such sums are callend <a href=\"https://en.wikipedia.org/wiki/Neumann_series\">Neumann Series</a>.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>We see that &#36;\\| A_k \\|&#36; looks like &#36;&#40;3/4&#41;^k&#36;, so the sum should exist, as:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\n\\| \\sum&#123;k&#61;0&#125;^n A^k \\| \\leq \\sum_&#123;k&#61;0&#125;^n \\|A^k\\| \\leq \\sum_&#123;k&#61;0&#125;^n \\|A\\|^k \\approx\n\\sum_&#123;k&#61;0&#125;^n &#40;3/4&#41;^k\n\\rightarrow 1 / &#40;1 - 3/4&#41;\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>In fact, we have more</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>Theorem on convergence of Neumann series</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Theorem &#40;p198&#41;. If &#36;\\|A \\| &lt; 1&#36;, then the matrix &#36;I -A&#36; is invertible and its inverse can be expressed as</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\n&#40;I-A&#41;^&#123;-1&#125; &#61; \\sum_k A^k.\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>Proof:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>First, the matrix is invertible. If not, there is a non-zero &#36;x&#36; where &#36;&#40;I-A&#41;x &#61; 0&#36;. We can suppose it is a unit vector. But then from &#36;x &#61; Ax&#36; we have</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\n1 &#61; \\|x \\| &#61; \\| Ax \\| \\leq \\|A \\| \\cdot \\| x \\| &#61; \\| A \\| &lt; 1.\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>This is of course a contradiction.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>To see that the sum is the correct one, we note this is basically the same as showing &#36;\\sum r^k &#61; 1/&#40;1-r&#41;&#36;, used above. Only instead of dividing, we multiply:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\n&#40;I -A&#41; \\sum^n A^k &#61; \\sum^n &#40;I - A&#41; A^k &#61; \\sum^n &#40;A^k - A^&#123;k&#43;1&#125;&#41; &#61;\n&#40;A^0 - A^1&#41; &#43; &#40;A^1 - A^2&#41; &#43; &#40;A^2 - A^3&#41; &#43; \\cdots &#43; &#40;A^&#123;n&#43;1&#125; - A^n&#41; &#61; I - A^&#123;n&#43;1&#125;.\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>But from &#36;\\|A^m\\| \\leq \\|A\\|^m&#36; we get the latter goes to &#36;0&#36;, and the convergence is to &#36;I&#36;.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Alternatively</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Thm: &#40;200&#41; Suppose &#36;A&#36; and &#36;B&#36; are &#36;n \\times n&#36; with &#36;\\| I - AB \\| &lt; 1&#36;, then <em>both</em> &#36;A&#36; and &#36;B&#36; are invertible and we can write as</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nA^&#123;-1&#125; &#61; B \\sum&#40;I - AB&#41;^k\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>Why? We can reexpress the previous one by saying &#36;A^&#123;-1&#125; &#61; \\sum &#40;I-A&#41;^k&#36;, under assumptions. Applying this to &#36;AB&#36; gives that under our assumption we have:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\n&#40;AB&#41;^&#123;-1&#125; &#61; \\sum &#40;I - AB&#41;^k\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>But multiplying both sides by &#36;B&#36; gives the right hand side, whereas &#36;B&#40;AB&#41;^&#123;-1&#125; &#61; &#40;BB^&#123;-1&#125;A^&#123;-1&#125;&#41; &#61; A^&#123;-1&#125;&#36;.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>Iteratively solving &#36;Ax &#61;b&#36;.</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Suppose we have an <em>approximate</em> solution, &#36;x^0&#36; to &#36;Ax&#61;b&#36; and &#36;A&#36; is invertible. Then:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nx &#61; A^&#123;-1&#125;b\\quad x^0 &#61; A^&#123;-1&#125; Ax^0.\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>And so, we can write:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nx &#61; x^0 &#43; A^&#123;-1&#125;&#40;b - Ax^0&#41; &#61; x^0 &#43; e^0,\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>Defining the error vector &#36;e^0&#36; as above. The residual vector is the difference between &#36;b&#36; and the value &#36;Ax^0&#36;, which id &#36;r^0 &#61; b - Ax^0&#36;.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The relationship between the error vector and the residual vector is:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\ne^0 &#61; A^&#123;-1&#125; r^0, \\quad\\text&#123;or &#125; Ae^0 &#61; r^0\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>Given the inputs, &#36;b&#36;, &#36;A&#36; and &#36;x^0&#36; we can compute &#36;r^0&#36; and then solve for &#36;e^0&#36;. This means we can refine our guess to give</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nx^1 &#61; x^0 &#43; e^0\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>If we expect round off errors or other errors, then this too will be an approximation. It should be a better one.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Example</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>From the book</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4x4 Array{Int64,2}:\n 420  210  140  105\n 210  140  105   84\n 140  105   84   70\n 105   84   70   60"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["A = [420 210 140 105; 210 140 105 84; 140 105 84 70; 105 84 70 60]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>and</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Int64,1}:\n 875\n 539\n 399\n 319"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["b = [875, 539, 399, 319]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>They claim this is a decent guess</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Float64,1}:\n 0.999988\n 1.00014 \n 0.99967 \n 1.00022 "]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["x0 = [0.999988, 1.000137, 0.99967, 1.000215]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>And indeed we have:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Float64,1}:\n -0.000105\n -7.0e-5  \n -3.5e-5  \n -4.8e-5  "]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["r0 = b - A*x0"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Can we refine it?</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Float64,1}:\n 1.0\n 1.0\n 1.0\n 1.0"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["e0 = A \\ r0\nx1 = x0 + e0"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>The answer is &#36;&#91;1,1,1,1&#93;&#36;. We aren&#39;t quite there:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Float64,1}:\n -1.11022e-15\n  3.73035e-14\n -1.18683e-13\n  8.81517e-14"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["x1 - [1,1,1,1]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>We try to refine it again:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Float64,1}:\n 1.0\n 1.0\n 1.0\n 1.0"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["r1 = b - A*x1\ne1 = A \\ r1\nx2 = x1 + e1"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>And now</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Float64,1}:\n -1.11022e-15\n  3.73035e-14\n -1.18683e-13\n  8.81517e-14"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["x2 - [1,1,1,1]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>So no better, as we got there in one step.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3></h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Suppose we have a <em>perturbed</em> inverse for &#36;A&#36;, &#36;B&#36;, which yields &#36;x^0 &#61; B b&#36; and is used for solving. &#40;This might be due just to round off.&#41;</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Then we have</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nx^1 &#61; x^0 &#43; e^0 &#61; x^0 &#43; Br^0 &#61; x^0 &#43; B&#40;b - Ax^0&#41;\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>And iterating:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nx^&#123;k&#43;1&#125; &#61; x^&#123;k&#125; &#43; e^&#123;k&#125; &#61; x^&#123;k&#125; &#43; Br^&#123;k&#125; &#61; x^&#123;k&#125; &#43; B&#40;b - Ax^k&#41;.\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>This says &#36;x^&#123;k&#43;1&#125; - x^k&#36; is &#36;x^0 - &#40;BA&#41;x^k&#36;</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>If &#36;B&#36; is close to &#36;A^&#123;-1&#125;&#36;, then we should have &#36;\\| I - BA\\| &lt; 1&#36;. So we can express &#36;A^&#123;-1&#125;&#36; in iterms of &#36;B&#36; via the previous formulas.</p>","metadata":{}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Thm &#40;P202&#41;. If &#36;\\| I - AB \\| &lt; 1&#36; then we have for &#36;m \\geq 0&#36;:</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nx^m &#61; B \\sum_&#123;k&#61;0&#125;^m &#40;I - AB&#41;^k b.\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>The partial sums on the right hand side converge to &#36;A^&#123;-1&#125;b &#61; x&#36;, so our iterative refinement converges to &#36;x&#36;.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Proof: We use induction. The case &#36;m&#61;0&#36; is just saying &#36;x^0 &#61; BIb&#36;, which is the definition of &#36;x^0&#36;.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Assuming this is true for case &#36;m&#36;, we need to show it try for &#36;m&#43;1&#36;. We note that the right hand side can be worked around to:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nB \\sum_&#123;k&#61;0&#125;^&#123;m&#43;1&#125; &#40;I - AB&#41;^k b &#61; Bb &#43; B\\sum_&#123;k&#61;1&#125;^&#123;M&#43;1&#125; &#40;I - AB&#41;^kb &#61; B&#40;b &#43; &#40;I-AB&#41;\\sum_&#123;k&#61;0&#125;^m &#40;I-AB&#41;^k b.\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>Now, starting from the left hand side:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\n\\begin&#123;align&#125;\nx^&#123;m&#43;1&#125;\n&amp;&#61; x^m &#43; B&#40;b - Ax^m&#41;\\\\\n&amp;&#61; B \\cdot \\sum_&#123;k&#61;0&#125;^m &#40;I-AB&#41;^k b &#43; B\\cdot &#40;b - A&#40;B \\sum_&#123;k&#61;0&#125;^m &#40;I-AB&#41;^k b&#41;&#41;\\\\\n&amp;&#61; B \\cdot &#40;b &#43; \\sum_&#123;k&#61;0&#125;^m &#40;I-AB&#41;^k b - AB \\cdot \\sum_&#123;k&#61;0&#125;^m &#40;I-AB&#41;^k b&#41;\\\\\n&amp;&#61; B \\cdot &#40;B &#43; &#40;I-AB&#41; \\cdot \\sum_&#123;k&#61;0&#125;^m &#40;I-AB&#41;^k b\\\\\n&amp;&#61; B \\sum_&#123;k&#61;0&#125;^&#123;m&#43;1&#125; &#40;I - AB&#41;^k b.\n\\end&#123;align&#125;\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<h2>Generalizations</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>We are again discussing indirect, iterative solutions to &#36;Ax&#61;b&#36;.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Suppose now &#36;B&#36; is not an approximate inverse, but just some matrix. Called &#36;Q&#36; in the book and given thename a splitting matrix. Then adding &#36;Qx&#36; to both sides of &#36;Ax &#61;b&#36; gives:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nQx &#61; &#40;Q-A&#41;x &#43; b\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>Which suggests an iterative scheme of the type</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nQ x^&#123;k&#43;1&#125; &#61; &#40;Q- A&#41;x^k &#43; b.\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>&#40;If &#36;Q^&#123;-1&#125; &#61; B&#36;, then multiplying both sides by &#36;B&#36; shows that our previous equation &#36;x^&#123;k&#43;1&#125; &#61; x^k &#43; B&#40;b-Ax^k&#41;&#36; is a special case.&#41;</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>For this to be good in general, we would want:</p>","metadata":{}},
{"cell_type":"markdown","source":"<ul>\n<li>the sequence &#36;x^k&#36; to be easy &#40;cheap&#41; to compute</li>\n<li>the sequence to converge rapidly</li>\n</ul>","metadata":{}},
{"cell_type":"markdown","source":"<p>In which case, we can solve.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>&#40;Suppose we had a large matrix, solving via &#36;LU&#36; takes &#36;n^3/3&#36; steps. If we can compute &#36;x^k&#36; cheaply, say order &#36;n^2&#36;, and convergence is rapid, this <em>could</em> be faster for large &#36;n&#36;.&#41;</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>Example</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Let &#36;A&#36; be the matrix:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n 0.611111\n 0.611111\n 0.611111"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["A= [1 1/2 1/3; 1/3 1 1/2; 1/2 1/3 1]\nb = [11, 11, 11]/18"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>We take &#36;Q&#36; to be the identify matrix, &#36;I&#36;:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3x3 Array{Float64,2}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["Q = eye(3)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>We check that &#36;\\| I - Q^&#123;-1&#125;A \\| &lt; 1&#36;:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8333333333333333"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["norm(I - A)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>So our convergence should hold.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>With this &#36;Q&#36;, our iteration step is just x^&#123;k&#43;1&#125; &#61; &#40;I-A&#41;x^&#123;k&#125; &#43; b &#61; x^k &#43; &#40;b - Ax^k&#41; &#61; x^k &#43; r^k</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>And we start at &#36;x&#61;&#91;0,0,0&#93;&#36;. What do 100 iterations produce:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n 0.611111\n 0.611111\n 0.611111"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["x = [0,0,0]\nr = b - A*x\nx = x + r"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>and again</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n 0.101852\n 0.101852\n 0.101852"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["r = b - A*x\nx = x+r"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Now we repeat 100 more times:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["([0.3333333305382701,0.3333333305382701,0.3333333305382701],[5.124282664858981e-9,5.124282775881284e-9,5.124282664858981e-9])"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["for k in 1:100\n  r = b - A*x \n  x = x + r\nend\nx, b - A*x"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Such a choice of &#36;Q&#36; is called the Richardson method.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Example</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>For a different example, take &#36;A&#36; by</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3x3 Array{Int64,2}:\n 2  -1   0\n 1   6  -2\n 4  -3   9"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["A = [2 -1 0; 1 6 -2; 4 -3 9]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>and &#36;b&#36;:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Int64,1}:\n  2\n -4\n  5"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["b = [2, -4, 5]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Now, let &#36;Q&#36; be the diagonal matrix of &#36;A&#36;. </p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3x3 Array{Int64,2}:\n 2  0  0\n 0  6  0\n 0  0  9"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["Q = diagm(diag(A))   # diag finds element, diagm makes matrix"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>We have</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.319649816948576"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["norm(1 - inv(Q)*A)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>So we should have convergence of the algorithm</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nQx^&#123;k&#43;1&#125; &#61; &#40;Q-A&#41;x^k &#43; b\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>If we start with &#36;x&#61;&#91;0,0,0&#93;&#36;, then our first step is given by</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n  1.0     \n -0.666667\n  0.555556"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["x = [0,0,0]\nx = Q \\ ((Q-A)*x + b)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>We repeat a few times:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n  0.675926 \n -0.814815 \n  0.0432099"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["x = Q \\ ((Q-A)*x + b)\nx = Q \\ ((Q-A)*x + b)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Are we close?</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n  0.166667\n -0.299383\n  0.537037"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["A*x - b"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Not really, let&#39;s repeat 20 times:</p>","metadata":{}},
{"outputs":[],"cell_type":"code","source":["for k in 1:20\n  x = Q \\ ((Q-A)*x + b)\nend"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>And check the residual</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n  7.64267e-9\n -2.52226e-8\n  5.48885e-8"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["A*x - b"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Another 20 times gets us closer:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n  8.88178e-16\n -8.88178e-16\n  4.44089e-15"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["for k in 1:20\n  x = Q \\ ((Q-A)*x + b)\nend\nA*x - b"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>For this method, called <em>Jacobi iteration</em> the solving part is trivial, as &#36;Q&#36; is diagonal. The multiplying by &#36;&#40;Q-A&#41;&#36; need not be costly for sparse matrices, so it could possibly be faster than the direct method of &#36;LU&#36; factorization.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Example</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>If we let &#36;Q&#36; be the lower triangular part of &#36;A&#36; we get the <em>Gauss-Seidel</em> method. Let&#39;s see that this converges as well:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>For our same A, we know define &#36;Q&#36; by:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3x3 Array{Int64,2}:\n 2   0  0\n 1   6  0\n 4  -3  9"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["Q = tril(A)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>We have</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5780112546868472"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["norm(I - inv(Q)*A)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>so convergence should occure.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>With a starting point at &#36;x&#61;&#91;0,0,0&#93;&#36; we dash off 25 iterations:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n -1.15941e-11\n  9.57945e-12\n  0.0        "]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["x = [0,0,0]\nfor k in 1:20\n  x = Q \\ ((Q-A)*x + b)\nend\nA*x - b"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>This method seems to converge faster than Jacobi iteration. It has other advantages, such as being able to be run in parallel.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>convergence of the method</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Thm. &#40;p210&#41; Suppose &#36;\\| I - Q^&#123;-1&#125;A\\| &lt; 1&#36; for some subordinate matrix notm. Then the sequence started at &#36;x^0&#36; will converge in the associated vector norm.</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<p>Pf. The algorithm starts from  &#36;Ax&#61;b&#36;, so if &#36;x&#36; is an actual solution, it is a fixed point of the algorithm. That is:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nQx &#61; &#40;Q-A&#41;x &#43; b, \\quad \\text&#123;and&#125; Qx^&#123;k&#43;1&#125; &#61; &#40;Q-A&#41;x^&#123;k&#125; &#43; b\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>Solving – mathematically – by multiplying by &#36;Q^&#123;-1&#125;&#36; reexpresses these as:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nx &#61; &#40;I - Q^&#123;-1&#125;A&#41;x &#43; Q^&#123;-1&#125;b \\text&#123;and&#125; x^&#123;k&#43;1&#125; &#61; &#40;I-Q^&#123;-1&#125;A&#41;x^&#123;k&#125; &#43; Q^&#123;-1&#125;b.\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>If we look at the difference vector</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nx^&#123;k&#43;1&#125; - x\n&#61;  &#40;I-Q^&#123;-1&#125;A&#41;x^&#123;k&#125; &#43; Q^&#123;-1&#125;b - &#40; &#40;I - Q^&#123;-1&#125;A&#41;x &#43; Q^&#123;-1&#125;b&#41;\n&#61;  &#40;I-Q^&#123;-1&#125;A&#41;&#40;x^&#123;k&#125; - x&#41;\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>So in norm, we have</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\n\\| x^&#123;k&#43;1&#125; -  x\\| \\leq \\| I - Q^&#123;-1&#125;A\\| \\|x^k - x\\|.\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>Which when iterated shows &#36; \\| x^&#123;k&#43;1&#125; -  x\\| \\rightarrow 0&#36;.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Now, we can say &#36;x&#36; exists because the assumption &#36;\\| I - Q^&#123;-1&#125;A\\| &lt; 1&#36; means that the &#36;Q^&#123;-1&#125;A&#36; is invertible, and hence so is &#36;A&#36;. So &#36;x &#61; A^&#123;-1&#125; b&#36;. Thus, any starting point will converge to &#36;x&#36;.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>An even more general case</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The following is an even more general iterative scheme:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nx^&#123;k&#43;1&#125; &#61; G x^k &#43; c\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>Where &#36;G&#36; is &#36;n \\times n&#36; and &#36;c&#36; is a vector in &#36;R^n&#36;. What conditions will ensure that this will converge?</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Eigenvalues</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The answer will involve the <em>eigenvalues</em> of a matrix &#36;A&#36;.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Recall, these are those &#36;\\lambda&#36; for which &#36;det&#40;A - \\lambda I&#41; &#61; 0&#36;, this being the characteristic equation of &#36;A&#36; and is a polynomial. These values may be complex values. The <em>spectral</em> radius is defined as the largest eigenvalue in magnitude:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\n\\rho&#40;A&#41;  &#61; \\max \\&#123; |\\lambda|: det&#40;A - \\lambda I&#41; &#61; 0\\&#125;\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Theorem: &#40;p214&#41; The spectral radius of &#36;A&#36; is the minimal value over all possible subordinate matrix norms.</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<p>This says that we know</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\n\\rho&#40;A&#41; \\leq \\| A\\|\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>for any subordinate matrix norm. And for and &#36;\\epsilon &gt;0&#36; there is some subordinate matrix norm with &#36;\\|A \\| \\leq \\rho&#40;A&#41; &#43; \\epsilon&#36;,</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Convergence</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The iteration</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nx^&#123;k&#43;1&#125; &#61;  Gx^k &#43; c\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>will produce a sequence converging to &#36;&#40;I-G&#41;^&#123;-1&#125;c&#36; for any starting vector iff and only if &#36;\\rho&#40;G&#41; &lt; 1&#36;.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Pf. We start by wrting</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\nx^k &#61; G^k x^0 &#43; \\sum_&#123;j&#61;0&#125;^&#123;k-1&#125; G^j c.\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>We know there is some matrix norm with &#36;\\| G \\| &lt; 1&#36; &#40;the is the minimal value part&#41;. For this norm, we have &#36;\\|G^kx^0\\| \\rightarrow 0&#36;.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The sum has a limit as &#36;k \\rightarrow \\infty&#36;, as the Neumann series theorem applies:</p>","metadata":{}},
{"cell_type":"markdown","source":"&#36;~\n\\sum_&#123;j&#61;0&#125;^\\infty G^j c &#61; &#40;I-G&#41;^&#123;-1&#125; c.\n~&#36;","metadata":{}},
{"cell_type":"markdown","source":"<p>Hence, as &#36;x^k \\rightarrow &#40;I-G&#41;^&#123;-1&#125;c&#36;.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>If &#36;\\rho&#40;G&#41; \\geq 1&#36;, then with &#36;x^0 &#61; 0&#36; we get &#36;x^k &#61; \\sum_&#123;j&#61;0&#125;^&#123;k-1&#125; G^j c&#36;. We can select &#36;\\lambda&#36; and &#36;u&#36; where &#36;Gu &#61; \\lambda u&#36; and &#36;|\\lambda| &gt; 1&#36;. Taking this as &#36;c&#36;, we get &#36;x^k &#61; \\sum_&#123;j&#61;0&#125;^&#123;k-1&#125; \\lambda^j u&#36; and this will diverge.</p>","metadata":{}}
    ],
 "metadata": {
  "language_info": {
   "name": "julia",
   "version": "0.4"
  },
 "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  }

 },
 "nbformat": 4,
 "nbformat_minor": 0

}
