{
  "cells": [
     {"cell_type":"markdown","source":"<h1>Neumann series and iterative methods</h1>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>When we have a norm, $\\| \\cdot \\|$, we can talk about <em>convergence</em> of a sequence of vectors, $v^k$ to a vector $v$ or convergence of matrices.</p>","metadata":{}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Fact: For a finite dimensional vector space if $v^k$ converges to $v$ with one norm, it will with another.</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<p>Which is not the case on infinite dimensional spaces such as the space of functions.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>Example, iterations of a matrix $A$</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Take $A$ to be a square matrix. Then we can form a series of matrices by</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nA^0, A^1, A^2, \\dots, A^n, \\dots\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>Example:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["2×2 Array{Float64,2}:\n 0.25  0.5 \n 0.5   0.25"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["A = (1/4) * [1 2; 2 1]"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Then we have</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["([0.25 0.5; 0.5 0.25], [0.3125 0.25; 0.25 0.3125], [0.203125 0.21875; 0.21875 0.203125], [0.160156 0.15625; 0.15625 0.160156])"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["A^1, A^2, A^3, A^4"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>The terms seem to be getting smaller.</p>","metadata":{}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Claim: $A^k \\rightarrow 0$.</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<p>Let's show the following: for a unit vector, we have $(4/3)^k A^k u \\rightarrow (1/2) [1,1]$.</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["2-element Array{Float64,1}:\n 0.333333\n 0.666667"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["n = 1; (4/3)^n * A^n * [1,0]"],"metadata":{},"execution_count":1},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["2-element Array{Float64,1}:\n 0.497942\n 0.502058"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["n =5; (4/3)^n * A^n * [1,0]"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>And jumping ahead:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["2-element Array{Float64,1}:\n 0.5\n 0.5"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["n = 20; (4/3)^n * A^n * [1,0]"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<blockquote>\n<p>Claim: $\\sum A_k$ exists.</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<p>Such sums are called <a href=\"https://en.wikipedia.org/wiki/Neumann_series\">Neumann Series</a>.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>We see that $\\| A_k \\|$ looks like $(3/4)^k$, so the sum should exist, as:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\n\\| \\sum_{k=0}^n A^k \\| \\leq \\sum_{k=0}^n \\|A^k\\| \\leq \\sum_{k=0}^n \\|A\\|^k \\approx\n\\sum_{k=0}^n (3/4)^k\n\\rightarrow 1 / (1 - 3/4)\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>In fact, we have more</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>Theorem on convergence of Neumann series</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Theorem (p198). If $\\|A \\| < 1$, then the matrix $I -A$ is invertible and its inverse can be expressed as</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\n(I-A)^{-1} = \\sum_k A^k.\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>Proof:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>First, the matrix is invertible. If not, there is a non-zero $x$ where $(I-A)x = 0$. We can suppose it is a unit vector. But then from $x = Ax$ we have</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\n1 = \\|x \\| = \\| Ax \\| \\leq \\|A \\| \\cdot \\| x \\| = \\| A \\| < 1.\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>This is of course a contradiction.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>To see that the sum is the correct one, we note this is basically the same as showing $\\sum r^k = 1/(1-r)$, used above. Only instead of dividing, we multiply:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\n(I -A) \\sum^n A^k = \\sum^n (I - A) A^k = \\sum^n (A^k - A^{k+1}) =\n(A^0 - A^1) + (A^1 - A^2) + (A^2 - A^3) + \\cdots + (A^{n+1} - A^n) = I - A^{n+1}.\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>But from $\\|A^m\\| \\leq \\|A\\|^m$ we get the latter goes to $0$, and the convergence is to $I$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Alternatively</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Thm: (p200) Suppose $A$ and $B$ are $n \\times n$ with $\\| I - AB \\| < 1$, then <em>both</em> $A$ and $B$ are invertible and we can write as</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nA^{-1} = B \\sum(I - AB)^k\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>Why? We can reexpress the previous one by saying $A^{-1} = \\sum (I-A)^k$, under assumptions. Applying this to $AB$ gives that under our assumption we have:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\n(AB)^{-1} = \\sum (I - AB)^k\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>But multiplying both sides by $B$ gives the right hand side, whereas $B(AB)^{-1} = (BB^{-1}A^{-1}) = A^{-1}$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>Iteratively solving $Ax =b$.</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Suppose we have an <em>approximate</em> solution, $x^0$ to $Ax=b$ and $A$ is invertible. Then:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nx = A^{-1}b\\quad x^0 = A^{-1} Ax^0.\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>And so, we can write:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nx = x^0 + A^{-1}(b - Ax^0) = x^0 + e^0,\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>Defining the error vector $e^0$ as above. The residual vector is the difference between $b$ and the value $Ax^0$, which id $r^0 = b - Ax^0$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The relationship between the error vector and the residual vector is:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\ne^0 = A^{-1} r^0, \\quad\\text{or } Ae^0 = r^0\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>Given the inputs, $b$, $A$ and $x^0$ we can compute $r^0$ and then solve for $e^0$. This means we can refine our guess to give</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nx^1 = x^0 + e^0\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>If we expect round off errors or other errors, then this too will be an approximation. It should be a better one.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Example</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>From the book</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4×4 Array{Int64,2}:\n 420  210  140  105\n 210  140  105   84\n 140  105   84   70\n 105   84   70   60"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["A = [420 210 140 105; 210 140 105 84; 140 105 84 70; 105 84 70 60]"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>and</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Int64,1}:\n 875\n 539\n 399\n 319"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["b = [875, 539, 399, 319]"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>They claim this is a decent guess</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Float64,1}:\n 0.999988\n 1.00014 \n 0.99967 \n 1.00022 "]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["x0 = [0.999988, 1.000137, 0.99967, 1.000215]"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>And indeed we have:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Float64,1}:\n -0.000105\n -7.0e-5  \n -3.5e-5  \n -4.8e-5  "]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["r0 = b - A*x0"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Can we refine it?</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Float64,1}:\n 1.0\n 1.0\n 1.0\n 1.0"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["e0 = A \\ r0\nx1 = x0 + e0"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>The answer is $[1,1,1,1]$. We aren't quite there:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Float64,1}:\n -1.11022e-15\n  3.73035e-14\n -1.18683e-13\n  8.81517e-14"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["x1 - [1,1,1,1]"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>We try to refine it again:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Float64,1}:\n 1.0\n 1.0\n 1.0\n 1.0"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["r1 = b - A*x1\ne1 = A \\ r1\nx2 = x1 + e1"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>And now</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Float64,1}:\n -1.11022e-15\n  3.73035e-14\n -1.18683e-13\n  8.81517e-14"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["x2 - [1,1,1,1]"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>So no better, as we got there in one step.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3></h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Suppose we have a <em>perturbed</em> inverse for $A$, $B$, which yields $x^0 = B b$ and is used for solving. (This might be due just to round off.)</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Then we have</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nx^1 = x^0 + e^0 = x^0 + Br^0 = x^0 + B(b - Ax^0)\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>And iterating:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nx^{k+1} = x^{k} + e^{k} = x^{k} + Br^{k} = x^{k} + B(b - Ax^k).\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>This says $x^{k+1} - x^k$ is $x^0 - (BA)x^k$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>If $B$ is close to $A^{-1}$, then we should have $\\| I - BA\\| < 1$. So we can express $A^{-1}$ in terms of $B$ via the previous formulas.</p>","metadata":{}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Thm (P202). If $\\| I - AB \\| < 1$ then we have for $m \\geq 0$:</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nx^m = B \\sum_{k=0}^m (I - AB)^k b.\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>The partial sums on the right hand side converge to $A^{-1}b = x$, so our iterative refinement converges to $x$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Proof: We use induction. The case $m=0$ is just saying $x^0 = BIb$, which is the definition of $x^0$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Assuming this is true for case $m$, we need to show it try for $m+1$. We note that the right hand side can be worked around to:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nB \\sum_{k=0}^{m+1} (I - AB)^k b = Bb + B\\sum_{k=1}^{M+1} (I - AB)^kb = B(b + (I-AB)\\sum_{k=0}^m (I-AB)^k b.\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>Now, starting from the left hand side:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\n\\begin{align}\nx^{m+1}\n&= x^m + B(b - Ax^m)\\\\\n&= B \\cdot \\sum_{k=0}^m (I-AB)^k b + B\\cdot (b - A(B \\sum_{k=0}^m (I-AB)^k b))\\\\\n&= B \\cdot (b + \\sum_{k=0}^m (I-AB)^k b - AB \\cdot \\sum_{k=0}^m (I-AB)^k b)\\\\\n&= B \\cdot (B + (I-AB) \\cdot \\sum_{k=0}^m (I-AB)^k b\\\\\n&= B \\sum_{k=0}^{m+1} (I - AB)^k b.\n\\end{align}\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<h2>Generalizations</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>We are again discussing indirect, iterative solutions to $Ax=b$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Suppose now $B$ is not an approximate inverse, but just some matrix. Called $Q$ in the book and given the name a splitting matrix. Then adding $Qx$ to both sides of $Ax =b$ gives:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nQx = (Q-A)x + b\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>Which suggests an iterative scheme of the type</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nQ x^{k+1} = (Q- A)x^k + b.\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>(If $Q^{-1} = B$, then multiplying both sides by $B$ shows that our previous equation $x^{k+1} = x^k + B(b-Ax^k)$ is a special case.)</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>For this to be good in general, we would want:</p>","metadata":{}},
{"cell_type":"markdown","source":"<ul>\n<li><p>the sequence $x^k$ to be easy (cheap) to compute</p>\n</li>\n<li><p>the sequence to converge rapidly</p>\n</li>\n</ul>","metadata":{}},
{"cell_type":"markdown","source":"<p>In which case, we can solve.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>(Suppose we had a large matrix, solving via $LU$ takes $n^3/3$ steps. If we can compute $x^k$ cheaply, say order $n^2$, and convergence is rapid, this <em>could</em> be faster for large $n$.)</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>Example</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Let $A$ be the matrix:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n 0.611111\n 0.611111\n 0.611111"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["A= [1 1/2 1/3; 1/3 1 1/2; 1/2 1/3 1]\nb = [11, 11, 11]/18"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>We take $Q$ to be the identify matrix, $I$:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3×3 Array{Float64,2}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["Q = eye(3)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>We check that $\\| I - Q^{-1}A \\| < 1$:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8333333333333333"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["norm(I - A)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>So our convergence should hold.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>With this $Q$, our iteration step is just $x^{k+1} = (I-A)x^{k} + b = x^k + (b - Ax^k) = x^k + r^k$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>And we start at $x=[0,0,0]$. What do 100 iterations produce:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n 0.611111\n 0.611111\n 0.611111"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["x = [0,0,0]\nr = b - A*x\nx = x + r"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>and again</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n 0.101852\n 0.101852\n 0.101852"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["r = b - A*x\nx = x+r"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Now we repeat 100 more times:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["([0.333333, 0.333333, 0.333333], [5.12428e-9, 5.12428e-9, 5.12428e-9])"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["for k in 1:100\n  r = b - A*x \n  x = x + r\nend\nx, b - A*x"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Such a choice of $Q$ is called the Richardson method.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Example</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>For a different example, take $A$ by</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3×3 Array{Int64,2}:\n 2  -1   0\n 1   6  -2\n 4  -3   9"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["A = [2 -1 0; 1 6 -2; 4 -3 9]"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>and $b$:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Int64,1}:\n  2\n -4\n  5"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["b = [2, -4, 5]"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Now, let $Q$ be the diagonal matrix of $A$. </p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3×3 Array{Int64,2}:\n 2  0  0\n 0  6  0\n 0  0  9"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["Q = diagm(diag(A))   # diag finds element, diagm makes matrix"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>We have</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6773603649878651"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["norm(I - inv(Q)*A)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>So we should have convergence of the algorithm</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nQx^{k+1} = (Q-A)x^k + b\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>If we start with $x=[0,0,0]$, then our first step is given by</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n  1.0     \n -0.666667\n  0.555556"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["x = [0,0,0]\nx = Q \\ ((Q-A)*x + b)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>We repeat a few times:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n  0.675926 \n -0.814815 \n  0.0432099"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["x = Q \\ ((Q-A)*x + b)\nx = Q \\ ((Q-A)*x + b)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Are we close?</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n  0.166667\n -0.299383\n  0.537037"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["A*x - b"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Not really, let's repeat 20 times:</p>","metadata":{}},
{"outputs":[],"cell_type":"code","source":["for k in 1:20\n  x = Q \\ ((Q-A)*x + b)\nend"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>And check the residual</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n  7.64267e-9\n -2.52226e-8\n  5.48885e-8"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["A*x - b"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Another 20 times gets us closer:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n  8.88178e-16\n -8.88178e-16\n  5.32907e-15"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["for k in 1:20\n  x = Q \\ ((Q-A)*x + b)\nend\nA*x - b"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>For this method, called <em>Jacobi iteration</em> the solving part is trivial, as $Q$ is diagonal. The multiplying by $(Q-A)$ need not be costly for sparse matrices, so it could possibly be faster than the direct method of $LU$ factorization.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Example</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>If we let $Q$ be the lower triangular part of $A$ we get the <em>Gauss-Seidel</em> method. Let's see that this converges as well:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>For our same A, we now define $Q$ by:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3×3 Array{Int64,2}:\n 2   0  0\n 1   6  0\n 4  -3  9"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["Q = tril(A)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>We have</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5780112546868472"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["norm(I - inv(Q)*A)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>so convergence should occur.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>With a starting point at $x=[0,0,0]$ we dash off $20$ iterations:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{Float64,1}:\n -1.15941e-11\n  9.57945e-12\n  0.0        "]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["x = [0,0,0]\nfor k in 1:20\n  x = Q \\ ((Q-A)*x + b)\nend\nA*x - b"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>This method seems to converge faster than Jacobi iteration. It has other advantages, such as being able to be run in parallel.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>convergence of the method</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Thm. (p210) Suppose $\\| I - Q^{-1}A\\| < 1$ for some subordinate matrix norm. Then the sequence started at $x^0$ will converge in the associated vector norm.</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<p>Pf. The algorithm starts from  $Ax=b$, so if $x$ is an actual solution, it is a fixed point of the algorithm. That is:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nQx = (Q-A)x + b, \\quad \\text{and } Qx^{k+1} = (Q-A)x^{k} + b\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>Solving – mathematically – by multiplying by $Q^{-1}$ reexpresses these as:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nx = (I - Q^{-1}A)x + Q^{-1}b \\text{ and } x^{k+1} = (I-Q^{-1}A)x^{k} + Q^{-1}b.\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>If we look at the difference vector</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nx^{k+1} - x\n=  (I-Q^{-1}A)x^{k} + Q^{-1}b - ( (I - Q^{-1}A)x + Q^{-1}b)\n=  (I-Q^{-1}A)(x^{k} - x)\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>So in norm, we have</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\n\\| x^{k+1} -  x\\| \\leq \\| I - Q^{-1}A\\| \\|x^k - x\\|.\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>Which when iterated shows $ \\| x^{k+1} -  x\\| \\rightarrow 0$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Now, we can say $x$ exists because the assumption $\\| I - Q^{-1}A\\| < 1$ means that the $Q^{-1}A$ is invertible, and hence so is $A$. So $x = A^{-1} b$. Thus, any starting point will converge to $x$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>An even more general case</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The following is an even more general iterative scheme:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nx^{k+1} = G x^k + c\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>Where $G$ is $n \\times n$ and $c$ is a vector in $R^n$. What conditions will ensure that this will converge?</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Eigenvalues</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The answer will involve the <em>eigenvalues</em> of a matrix $A$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Recall, these are those $\\lambda$ for which $\\det(A - \\lambda I) = 0$, this being the characteristic equation of $A$ and is a polynomial. These values may be complex values. The <em>spectral</em> radius is defined as the largest eigenvalue in magnitude:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\n\\rho(A)  = \\max \\{ |\\lambda|: \\det(A - \\lambda I) = 0\\}\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<blockquote>\n<p>Theorem: (p214) The spectral radius of $A$ is the minimal value over all possible subordinate matrix norms.</p>\n</blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<p>This says that we know</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\n\\rho(A) \\leq \\| A\\|\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>for any subordinate matrix norm. And for and $\\epsilon >0$ there is some subordinate matrix norm with $\\|A \\| \\leq \\rho(A) + \\epsilon$,</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Convergence</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The iteration</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nx^{k+1} =  Gx^k + c\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>will produce a sequence converging to $(I-G)^{-1}c$ for any starting vector iff and only if $\\rho(G) < 1$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Pf. We start by writing</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\nx^k = G^k x^0 + \\sum_{j=0}^{k-1} G^j c.\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>We know there is some matrix norm with $\\| G \\| < 1$ (the is the minimal value part). For this norm, we have $\\|G^kx^0\\| \\rightarrow 0$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The sum has a limit as $k \\rightarrow \\infty$, as the Neumann series theorem applies:</p>","metadata":{}},
{"cell_type":"markdown","source":"\n$$\n\\sum_{j=0}^\\infty G^j c = (I-G)^{-1} c.\n$$\n","metadata":{}},
{"cell_type":"markdown","source":"<p>Hence, as $x^k \\rightarrow (I-G)^{-1}c$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>If $\\rho(G) \\geq 1$, then with $x^0 = 0$ we get $x^k = \\sum_{j=0}^{k-1} G^j c$. We can select $\\lambda$ and $u$ where $Gu = \\lambda u$ and $|\\lambda| > 1$. Taking this as $c$, we get $x^k = \\sum_{j=0}^{k-1} \\lambda^j u$ and this will diverge.</p>","metadata":{}}
    ],
 "metadata": {
  "language_info": {
   "file_extension": ".jl",                                                                                                             
   "mimetype": "application/julia", 
   "name": "julia",
   "version": "0.6"
  },
 "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  }

 },
 "nbformat": 4,
 "nbformat_minor": 2

}
