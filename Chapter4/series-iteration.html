
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"> 
    <meta name="viewport" content="width=device-width, initial-scale=1">




<link
  href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css"
  rel="stylesheet">

<style>
.julia {font-family: "Source Code Pro";
        color:#0033CC;
        }
body { padding-top: 60px; }
h5:before {content:"\2746\ ";}
h6:before {content:"\2742\ ";}
pre {display: block;}
</style>

<script src="http://code.jquery.com/jquery.js"></script>
<script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>

<!-- .julia:before {content: "julia> "} -->

<style></style>

<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>


<!-- not TeX-AMS-MML_HTMLorMML-->
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG">  
</script>
<script>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ["\$","\$"], ["\\(","\\)"]]
  },
  displayAlign: "left",
  displayIndent: "5%"
});
</script>


<script type="text/javascript">
$( document ).ready(function() {
  $("h1").each(function(index) { 
       var title = $( this ).text()
       $("#page_title").html("<strong>" + title + "</strong>");
       document.title = title
  });
  $( "h2" ).each(function( index ) {
    var nm =  $( this ).text();                                    
    var id = $.trim(nm).replace(/ /g,'');
    this.id = id
    $("#page_dropdown").append("<li><a href='#" + id + "'>" + nm + "</a></li>");
  });
  $('[data-toggle="popover"]').popover();  
});
</script>

</head>


<body data-spy="scroll" >

<nav class="navbar navbar-default  navbar-fixed-top">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
         
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li><a href="#" id="page_title"></a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
         <li class="dropdown">
           <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
           Jump to... <span class="caret"></span></a>
          <ul class="dropdown-menu" role="menu" id="page_dropdown"></ul>
        </li>
      </ul>
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

<header>
</header>

<div class="container-fluid">
  <div class="span10 offset1">
<h1>Neumann series and iterative methods</h1><p>When we have a norm, $\| \cdot \|$, we can talk about <em>convergence</em> of a sequence of vectors, $v^k$ to a vector $v$ or convergence of matrices.</p><blockquote>
<p>Fact: For a finite dimensional vector space if $v^k$ converges to $v$ with one norm, it will with another.</p>
</blockquote><p>Which is not the case on infinite dimensional spaces such as the space of functions.</p><h2>Example, iterations of a matrix $A$</h2><p>Take $A$ to be a square matrix. Then we can form a series of matrices by</p>$$~
A^0, A^1, A^2, \dots, A^n, \dots
~$$
<p>Example:</p><pre class="sourceCode julia">A = (1/4) * [1 2; 2 1]</pre>
<pre class="output">
2×2 Array{Float64,2}:
 0.25  0.5 
 0.5   0.25</pre>

<p>Then we have</p><pre class="sourceCode julia">A^1, A^2, A^3, A^4</pre>
<pre class="output">
([0.25 0.5; 0.5 0.25], [0.3125 0.25; 0.25 0.3125], [0.203125 0.21875; 0.21875 0.203125], [0.160156 0.15625; 0.15625 0.160156])</pre>

<p>The terms seem to be getting smaller.</p><blockquote>
<p>Claim: $A^k \rightarrow 0$.</p>
</blockquote><p>Let's show the following: for a unit vector, we have $(4/3)^k A^k u \rightarrow (1/2) [1,1]$.</p><pre class="sourceCode julia">n = 1; (4/3)^n * A^n * [1,0]</pre>
<pre class="output">
2-element Array{Float64,1}:
 0.333333
 0.666667</pre>

<pre class="sourceCode julia">n =5; (4/3)^n * A^n * [1,0]</pre>
<pre class="output">
2-element Array{Float64,1}:
 0.497942
 0.502058</pre>

<p>And jumping ahead:</p><pre class="sourceCode julia">n = 20; (4/3)^n * A^n * [1,0]</pre>
<pre class="output">
2-element Array{Float64,1}:
 0.5
 0.5</pre>

<blockquote>
<p>Claim: $\sum A_k$ exists.</p>
</blockquote><p>Such sums are called <a href="https://en.wikipedia.org/wiki/Neumann_series">Neumann Series</a>.</p><p>We see that $\| A_k \|$ looks like $(3/4)^k$, so the sum should exist, as:</p>$$~
\| \sum_{k=0}^n A^k \| \leq \sum_{k=0}^n \|A^k\| \leq \sum_{k=0}^n \|A\|^k \approx
\sum_{k=0}^n (3/4)^k
\rightarrow 1 / (1 - 3/4)
~$$
<p>In fact, we have more</p><h2>Theorem on convergence of Neumann series</h2><blockquote>
<p>Theorem (p198). If $\|A \| < 1$, then the matrix $I -A$ is invertible and its inverse can be expressed as</p>
</blockquote>$$~
(I-A)^{-1} = \sum_k A^k.
~$$
<p>Proof:</p><p>First, the matrix is invertible. If not, there is a non-zero $x$ where $(I-A)x = 0$. We can suppose it is a unit vector. But then from $x = Ax$ we have</p>$$~
1 = \|x \| = \| Ax \| \leq \|A \| \cdot \| x \| = \| A \| < 1.
~$$
<p>This is of course a contradiction.</p><p>To see that the sum is the correct one, we note this is basically the same as showing $\sum r^k = 1/(1-r)$, used above. Only instead of dividing, we multiply:</p>$$~
(I -A) \sum^n A^k = \sum^n (I - A) A^k = \sum^n (A^k - A^{k+1}) =
(A^0 - A^1) + (A^1 - A^2) + (A^2 - A^3) + \cdots + (A^{n+1} - A^n) = I - A^{n+1}.
~$$
<p>But from $\|A^m\| \leq \|A\|^m$ we get the latter goes to $0$, and the convergence is to $I$.</p><h3>Alternatively</h3><blockquote>
<p>Thm: (p200) Suppose $A$ and $B$ are $n \times n$ with $\| I - AB \| < 1$, then <em>both</em> $A$ and $B$ are invertible and we can write as</p>
</blockquote>$$~
A^{-1} = B \sum(I - AB)^k
~$$
<p>Why? We can reexpress the previous one by saying $A^{-1} = \sum (I-A)^k$, under assumptions. Applying this to $AB$ gives that under our assumption we have:</p>$$~
(AB)^{-1} = \sum (I - AB)^k
~$$
<p>But multiplying both sides by $B$ gives the right hand side, whereas $B(AB)^{-1} = (BB^{-1}A^{-1}) = A^{-1}$.</p><h2>Iteratively solving $Ax =b$.</h2><p>Suppose we have an <em>approximate</em> solution, $x^0$ to $Ax=b$ and $A$ is invertible. Then:</p>$$~
x = A^{-1}b\quad x^0 = A^{-1} Ax^0.
~$$
<p>And so, we can write:</p>$$~
x = x^0 + A^{-1}(b - Ax^0) = x^0 + e^0,
~$$
<p>Defining the error vector $e^0$ as above. The residual vector is the difference between $b$ and the value $Ax^0$, which id $r^0 = b - Ax^0$.</p><p>The relationship between the error vector and the residual vector is:</p>$$~
e^0 = A^{-1} r^0, \quad\text{or } Ae^0 = r^0
~$$
<p>Given the inputs, $b$, $A$ and $x^0$ we can compute $r^0$ and then solve for $e^0$. This means we can refine our guess to give</p>$$~
x^1 = x^0 + e^0
~$$
<p>If we expect round off errors or other errors, then this too will be an approximation. It should be a better one.</p><h3>Example</h3><p>From the book</p><pre class="sourceCode julia">A = [420 210 140 105; 210 140 105 84; 140 105 84 70; 105 84 70 60]</pre>
<pre class="output">
4×4 Array{Int64,2}:
 420  210  140  105
 210  140  105   84
 140  105   84   70
 105   84   70   60</pre>

<p>and</p><pre class="sourceCode julia">b = [875, 539, 399, 319]</pre>
<pre class="output">
4-element Array{Int64,1}:
 875
 539
 399
 319</pre>

<p>They claim this is a decent guess</p><pre class="sourceCode julia">x0 = [0.999988, 1.000137, 0.99967, 1.000215]</pre>
<pre class="output">
4-element Array{Float64,1}:
 0.999988
 1.00014 
 0.99967 
 1.00022 </pre>

<p>And indeed we have:</p><pre class="sourceCode julia">r0 = b - A*x0</pre>
<pre class="output">
4-element Array{Float64,1}:
 -0.000105
 -7.0e-5  
 -3.5e-5  
 -4.8e-5  </pre>

<p>Can we refine it?</p><pre class="sourceCode julia">e0 = A \ r0
x1 = x0 + e0</pre>
<pre class="output">
4-element Array{Float64,1}:
 1.0
 1.0
 1.0
 1.0</pre>

<p>The answer is $[1,1,1,1]$. We aren't quite there:</p><pre class="sourceCode julia">x1 - [1,1,1,1]</pre>
<pre class="output">
4-element Array{Float64,1}:
 -1.11022e-15
  3.73035e-14
 -1.18683e-13
  8.81517e-14</pre>

<p>We try to refine it again:</p><pre class="sourceCode julia">r1 = b - A*x1
e1 = A \ r1
x2 = x1 + e1</pre>
<pre class="output">
4-element Array{Float64,1}:
 1.0
 1.0
 1.0
 1.0</pre>

<p>And now</p><pre class="sourceCode julia">x2 - [1,1,1,1]</pre>
<pre class="output">
4-element Array{Float64,1}:
 -1.11022e-15
  3.73035e-14
 -1.18683e-13
  8.81517e-14</pre>

<p>So no better, as we got there in one step.</p><h3></h3><p>Suppose we have a <em>perturbed</em> inverse for $A$, $B$, which yields $x^0 = B b$ and is used for solving. (This might be due just to round off.)</p><p>Then we have</p>$$~
x^1 = x^0 + e^0 = x^0 + Br^0 = x^0 + B(b - Ax^0)
~$$
<p>And iterating:</p>$$~
x^{k+1} = x^{k} + e^{k} = x^{k} + Br^{k} = x^{k} + B(b - Ax^k).
~$$
<p>This says $x^{k+1} - x^k$ is $x^0 - (BA)x^k$</p><p>If $B$ is close to $A^{-1}$, then we should have $\| I - BA\| < 1$. So we can express $A^{-1}$ in terms of $B$ via the previous formulas.</p><blockquote>
<p>Thm (P202). If $\| I - AB \| < 1$ then we have for $m \geq 0$:</p>
</blockquote>$$~
x^m = B \sum_{k=0}^m (I - AB)^k b.
~$$
<p>The partial sums on the right hand side converge to $A^{-1}b = x$, so our iterative refinement converges to $x$.</p><p>Proof: We use induction. The case $m=0$ is just saying $x^0 = BIb$, which is the definition of $x^0$.</p><p>Assuming this is true for case $m$, we need to show it try for $m+1$. We note that the right hand side can be worked around to:</p>$$~
B \sum_{k=0}^{m+1} (I - AB)^k b = Bb + B\sum_{k=1}^{M+1} (I - AB)^kb = B(b + (I-AB)\sum_{k=0}^m (I-AB)^k b.
~$$
<p>Now, starting from the left hand side:</p>$$~
\begin{align}
x^{m+1}
&= x^m + B(b - Ax^m)\\
&= B \cdot \sum_{k=0}^m (I-AB)^k b + B\cdot (b - A(B \sum_{k=0}^m (I-AB)^k b))\\
&= B \cdot (b + \sum_{k=0}^m (I-AB)^k b - AB \cdot \sum_{k=0}^m (I-AB)^k b)\\
&= B \cdot (B + (I-AB) \cdot \sum_{k=0}^m (I-AB)^k b\\
&= B \sum_{k=0}^{m+1} (I - AB)^k b.
\end{align}
~$$
<h2>Generalizations</h2><p>We are again discussing indirect, iterative solutions to $Ax=b$.</p><p>Suppose now $B$ is not an approximate inverse, but just some matrix. Called $Q$ in the book and given the name a splitting matrix. Then adding $Qx$ to both sides of $Ax =b$ gives:</p>$$~
Qx = (Q-A)x + b
~$$
<p>Which suggests an iterative scheme of the type</p>$$~
Q x^{k+1} = (Q- A)x^k + b.
~$$
<p>(If $Q^{-1} = B$, then multiplying both sides by $B$ shows that our previous equation $x^{k+1} = x^k + B(b-Ax^k)$ is a special case.)</p><p>For this to be good in general, we would want:</p><ul>
<li><p>the sequence $x^k$ to be easy (cheap) to compute</p>
</li>
<li><p>the sequence to converge rapidly</p>
</li>
</ul><p>In which case, we can solve.</p><p>(Suppose we had a large matrix, solving via $LU$ takes $n^3/3$ steps. If we can compute $x^k$ cheaply, say order $n^2$, and convergence is rapid, this <em>could</em> be faster for large $n$.)</p><h2>Example</h2><p>Let $A$ be the matrix:</p><pre class="sourceCode julia">A= [1 1/2 1/3; 1/3 1 1/2; 1/2 1/3 1]
b = [11, 11, 11]/18</pre>
<pre class="output">
3-element Array{Float64,1}:
 0.611111
 0.611111
 0.611111</pre>

<p>We take $Q$ to be the identify matrix, $I$:</p><pre class="sourceCode julia">Q = eye(3)</pre>
<pre class="output">
3×3 Array{Float64,2}:
 1.0  0.0  0.0
 0.0  1.0  0.0
 0.0  0.0  1.0</pre>

<p>We check that $\| I - Q^{-1}A \| < 1$:</p><pre class="sourceCode julia">norm(I - A)</pre>
<pre class="output">
0.8333333333333333</pre>

<p>So our convergence should hold.</p><p>With this $Q$, our iteration step is just $x^{k+1} = (I-A)x^{k} + b = x^k + (b - Ax^k) = x^k + r^k$</p><p>And we start at $x=[0,0,0]$. What do 100 iterations produce:</p><pre class="sourceCode julia">x = [0,0,0]
r = b - A*x
x = x + r</pre>
<pre class="output">
3-element Array{Float64,1}:
 0.611111
 0.611111
 0.611111</pre>

<p>and again</p><pre class="sourceCode julia">r = b - A*x
x = x+r</pre>
<pre class="output">
3-element Array{Float64,1}:
 0.101852
 0.101852
 0.101852</pre>

<p>Now we repeat 100 more times:</p><pre class="sourceCode julia">for k in 1:100
  r = b - A*x 
  x = x + r
end
x, b - A*x</pre>
<pre class="output">
([0.333333, 0.333333, 0.333333], [5.12428e-9, 5.12428e-9, 5.12428e-9])</pre>

<p>Such a choice of $Q$ is called the Richardson method.</p><h3>Example</h3><p>For a different example, take $A$ by</p><pre class="sourceCode julia">A = [2 -1 0; 1 6 -2; 4 -3 9]</pre>
<pre class="output">
3×3 Array{Int64,2}:
 2  -1   0
 1   6  -2
 4  -3   9</pre>

<p>and $b$:</p><pre class="sourceCode julia">b = [2, -4, 5]</pre>
<pre class="output">
3-element Array{Int64,1}:
  2
 -4
  5</pre>

<p>Now, let $Q$ be the diagonal matrix of $A$. </p><pre class="sourceCode julia">Q = diagm(diag(A))   # diag finds element, diagm makes matrix</pre>
<pre class="output">
3×3 Array{Int64,2}:
 2  0  0
 0  6  0
 0  0  9</pre>

<p>We have</p><pre class="sourceCode julia">norm(I - inv(Q)*A)</pre>
<pre class="output">
0.6773603649878651</pre>

<p>So we should have convergence of the algorithm</p>$$~
Qx^{k+1} = (Q-A)x^k + b
~$$
<p>If we start with $x=[0,0,0]$, then our first step is given by</p><pre class="sourceCode julia">x = [0,0,0]
x = Q \ ((Q-A)*x + b)</pre>
<pre class="output">
3-element Array{Float64,1}:
  1.0     
 -0.666667
  0.555556</pre>

<p>We repeat a few times:</p><pre class="sourceCode julia">x = Q \ ((Q-A)*x + b)
x = Q \ ((Q-A)*x + b)</pre>
<pre class="output">
3-element Array{Float64,1}:
  0.675926 
 -0.814815 
  0.0432099</pre>

<p>Are we close?</p><pre class="sourceCode julia">A*x - b</pre>
<pre class="output">
3-element Array{Float64,1}:
  0.166667
 -0.299383
  0.537037</pre>

<p>Not really, let's repeat 20 times:</p><pre class="sourceCode julia">for k in 1:20
  x = Q \ ((Q-A)*x + b)
end</pre>
<p>And check the residual</p><pre class="sourceCode julia">A*x - b</pre>
<pre class="output">
3-element Array{Float64,1}:
  7.64267e-9
 -2.52226e-8
  5.48885e-8</pre>

<p>Another 20 times gets us closer:</p><pre class="sourceCode julia">for k in 1:20
  x = Q \ ((Q-A)*x + b)
end
A*x - b</pre>
<pre class="output">
3-element Array{Float64,1}:
  8.88178e-16
 -8.88178e-16
  5.32907e-15</pre>

<p>For this method, called <em>Jacobi iteration</em> the solving part is trivial, as $Q$ is diagonal. The multiplying by $(Q-A)$ need not be costly for sparse matrices, so it could possibly be faster than the direct method of $LU$ factorization.</p><h3>Example</h3><p>If we let $Q$ be the lower triangular part of $A$ we get the <em>Gauss-Seidel</em> method. Let's see that this converges as well:</p><p>For our same A, we now define $Q$ by:</p><pre class="sourceCode julia">Q = tril(A)</pre>
<pre class="output">
3×3 Array{Int64,2}:
 2   0  0
 1   6  0
 4  -3  9</pre>

<p>We have</p><pre class="sourceCode julia">norm(I - inv(Q)*A)</pre>
<pre class="output">
0.5780112546868472</pre>

<p>so convergence should occur.</p><p>With a starting point at $x=[0,0,0]$ we dash off $20$ iterations:</p><pre class="sourceCode julia">x = [0,0,0]
for k in 1:20
  x = Q \ ((Q-A)*x + b)
end
A*x - b</pre>
<pre class="output">
3-element Array{Float64,1}:
 -1.15941e-11
  9.57945e-12
  0.0        </pre>

<p>This method seems to converge faster than Jacobi iteration. It has other advantages, such as being able to be run in parallel.</p><h3>convergence of the method</h3><blockquote>
<p>Thm. (p210) Suppose $\| I - Q^{-1}A\| < 1$ for some subordinate matrix norm. Then the sequence started at $x^0$ will converge in the associated vector norm.</p>
</blockquote><p>Pf. The algorithm starts from  $Ax=b$, so if $x$ is an actual solution, it is a fixed point of the algorithm. That is:</p>$$~
Qx = (Q-A)x + b, \quad \text{and } Qx^{k+1} = (Q-A)x^{k} + b
~$$
<p>Solving – mathematically – by multiplying by $Q^{-1}$ reexpresses these as:</p>$$~
x = (I - Q^{-1}A)x + Q^{-1}b \text{ and } x^{k+1} = (I-Q^{-1}A)x^{k} + Q^{-1}b.
~$$
<p>If we look at the difference vector</p>$$~
x^{k+1} - x
=  (I-Q^{-1}A)x^{k} + Q^{-1}b - ( (I - Q^{-1}A)x + Q^{-1}b)
=  (I-Q^{-1}A)(x^{k} - x)
~$$
<p>So in norm, we have</p>$$~
\| x^{k+1} -  x\| \leq \| I - Q^{-1}A\| \|x^k - x\|.
~$$
<p>Which when iterated shows $ \| x^{k+1} -  x\| \rightarrow 0$.</p><p>Now, we can say $x$ exists because the assumption $\| I - Q^{-1}A\| < 1$ means that the $Q^{-1}A$ is invertible, and hence so is $A$. So $x = A^{-1} b$. Thus, any starting point will converge to $x$.</p><h2>An even more general case</h2><p>The following is an even more general iterative scheme:</p>$$~
x^{k+1} = G x^k + c
~$$
<p>Where $G$ is $n \times n$ and $c$ is a vector in $R^n$. What conditions will ensure that this will converge?</p><h3>Eigenvalues</h3><p>The answer will involve the <em>eigenvalues</em> of a matrix $A$.</p><p>Recall, these are those $\lambda$ for which $\det(A - \lambda I) = 0$, this being the characteristic equation of $A$ and is a polynomial. These values may be complex values. The <em>spectral</em> radius is defined as the largest eigenvalue in magnitude:</p>$$~
\rho(A)  = \max \{ |\lambda|: \det(A - \lambda I) = 0\}
~$$
<blockquote>
<p>Theorem: (p214) The spectral radius of $A$ is the minimal value over all possible subordinate matrix norms.</p>
</blockquote><p>This says that we know</p>$$~
\rho(A) \leq \| A\|
~$$
<p>for any subordinate matrix norm. And for and $\epsilon >0$ there is some subordinate matrix norm with $\|A \| \leq \rho(A) + \epsilon$,</p><h3>Convergence</h3><p>The iteration</p>$$~
x^{k+1} =  Gx^k + c
~$$
<p>will produce a sequence converging to $(I-G)^{-1}c$ for any starting vector iff and only if $\rho(G) < 1$.</p><p>Pf. We start by writing</p>$$~
x^k = G^k x^0 + \sum_{j=0}^{k-1} G^j c.
~$$
<p>We know there is some matrix norm with $\| G \| < 1$ (the is the minimal value part). For this norm, we have $\|G^kx^0\| \rightarrow 0$.</p><p>The sum has a limit as $k \rightarrow \infty$, as the Neumann series theorem applies:</p>$$~
\sum_{j=0}^\infty G^j c = (I-G)^{-1} c.
~$$
<p>Hence, as $x^k \rightarrow (I-G)^{-1}c$.</p><p>If $\rho(G) \geq 1$, then with $x^0 = 0$ we get $x^k = \sum_{j=0}^{k-1} G^j c$. We can select $\lambda$ and $u$ where $Gu = \lambda u$ and $|\lambda| > 1$. Taking this as $c$, we get $x^k = \sum_{j=0}^{k-1} \lambda^j u$ and this will diverge.</p>
  </div>
</div>  

</body>
</html>
