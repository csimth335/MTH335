
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"> 
    <meta name="viewport" content="width=device-width, initial-scale=1">




<link
  href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css"
  rel="stylesheet">

<style>
.julia {font-family: "Source Code Pro";
        color:#0033CC;
        }
body { padding-top: 60px; }
h5:before {content:"\2746\ ";}
h6:before {content:"\2742\ ";}
pre {display: block;}
</style>

<script src="http://code.jquery.com/jquery.js"></script>
<script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>

<!-- .julia:before {content: "julia> "} -->

<style></style>

<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>


<!-- not TeX-AMS-MML_HTMLorMML-->
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG">  
</script>
<script>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ["\$","\$"], ["\\(","\\)"]]
  },
  displayAlign: "left",
  displayIndent: "5%"
});
</script>


<script type="text/javascript">
$( document ).ready(function() {
  $("h1").each(function(index) { 
       var title = $( this ).text()
       $("#page_title").html("<strong>" + title + "</strong>");
       document.title = title
  });
  $( "h2" ).each(function( index ) {
    var nm =  $( this ).text();                                    
    var id = $.trim(nm).replace(/ /g,'');
    this.id = id
    $("#page_dropdown").append("<li><a href='#" + id + "'>" + nm + "</a></li>");
  });
  $('[data-toggle="popover"]').popover();  
});
</script>

</head>


<body data-spy="scroll" >

<nav class="navbar navbar-default  navbar-fixed-top">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
         
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li><a href="#" id="page_title"></a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
         <li class="dropdown">
           <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
           Jump to... <span class="caret"></span></a>
          <ul class="dropdown-menu" role="menu" id="page_dropdown"></ul>
        </li>
      </ul>
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

<header>
</header>

<div class="container-fluid">
  <div class="span10 offset1">
<h1>LU decomposition and floating point issues</h1><p>Solving $A x = b$ for $x$ with inputs $A$ and $b$ can be done more efficiently by $LU$ factorization, than by finding an inverse and then solving via $x = A^{-1}b$. The number of steps is at least 1/3 as few.</p><p>However, there are still many steps needed ($n^3/3$) and we know that at each floating point operation we have:</p>$$~
fl(x \odot y) = (x \odot y) (1 + \delta)
~$$
<p>Where $\delta \leq \epsilon$, the machine tolerance number. So, even if each operation introduces an error that is bounded, when there are many operations these errors can accumulate. How much?</p><h2>Recall</h2><p>We have seen such a question before in Theorem 1 on page 49:</p><blockquote>
<p>Theorem: If $x_1, x_2, \dots, x_n$ are positive machine numbers with unit roundoff $\epsilon$ then the <em>relative</em> roundoff error in computing their sum is at most $(1+\epsilon)^n \approx n \epsilon$.</p>
</blockquote><p>A near analog is this theorem from p249 (modified and simplified):</p><blockquote>
<p>Theorem 2 p 249. Suppose $x_1, \dots, x_n$ and $y_1, \dots, y_n$ are machine numbers. (We further suppose these are positive. Consider the sum $\sum^n x_i y_i$ (the dot product). If we compute in the natural way, then the machine value is bounded by the mathematical value times $(6/5\cdot(n+1)\epsilon)$.</p>
</blockquote><p>By the natural way, we mean $z_k = fl(z_{k-1} + fl(x_ky_k))$.</p><h2>LU Factorization</h2><p>Let's look at the LU factorization with partial pivoting. The algorithm produces numbers $a_{ij}^{(k)}$ and $l_{ik}$ for $k=0,1,\dots,n$ steps.</p><p>Where we have for the case $i > k$ and $j>k$</p>$$~
a_{ij}^{(k+1)} = a_{ij}^{(k)} - l_{ik}a_{kj}^{(k)}, \quad\text{and }
l_{ik} = \frac{a_{ik}^{(k)} }{a_{kk}^{(k)}}.
~$$
<p>With floating point concerns, this becomes:</p>$$~
\tilde{a}_{ij}^{(k+1)} =
 fl(\tilde{a}_{ij}^{(k)} -  fl( \tilde{l}_{ik}\tilde{a}_{kj}^{(k)})), \quad\text{ and }
\tilde{l}_{ik} = fl(\frac{\tilde{a}_{ik}^{(k)} }{\tilde{a}_{kk}^{(k)}}). 
~$$
<p>So, the end of the process we have two matrices $\tilde{L}$ and $\tilde{U}$ instead of the mathematical $L$ and $U$. How far away will these be?</p><blockquote>
<p>Theorem (p247). Under some pivoting assumptions</p>
</blockquote>$$~
\tilde{L} \tilde{U} = A + E
~$$
<p>Where the entries of $E$ are bounded by:</p>$$~
|e_{ij}| \leq 2n\epsilon \max |a_{ik}^{(k)}|.
~$$
<h2>And then...</h2><p>We solve $Ax=b$ by solving $Ly=b$ and then $Ux=y$. But secretly we solve $\tilde{L}y=b$. But the substitution will introduce errors. How big?</p><blockquote>
<p>Thm 3 p250. If $L$ is a $n\times n$ lower triangular matrix of machine numbers (like $\tilde{L}$) and $b$ a vector of machine numbers and $y$ is computed by substituting with $L$ and $b$, then the resulting vector, $\tilde{y}$ is subject to rounding, so may not exactly solve $Ly=b$. It does solve</p>
</blockquote>$$~
(L+\Delta) \tilde{y} = b
~$$
<p>Where the entries of $\Delta$ satisfy:</p>$$~
|\Delta_{ij}| \leq \frac{6}{5}(n+1) \epsilon |l_{ij}|.
~$$
<p>A similar theorem applies for solving $Uy=b$, though the bound  includes $|u_{ij}|$ terms.</p><h3>But with our pivoting...</h3><p>With our pivoting we have the entries of $L$ are bounded by $1$, as we pivot to make the row chosen have the largest value and our entries involve $a_{ik}^{(k)} / a_{kk}^{(k)}$. This isn't the case for $U$ and $U$ can grow. For example, consider this <a href="http://www.math.iit.edu/~fass/477577_Chapter_7.pdf">matrix</a>:</p><pre class="sourceCode julia">n = 5
A = I-tril(ones(Int, n,n),-1)
A[:,n] = ones(Int, n)
A</pre>
<pre class="output">
5×5 Array{Int64,2}:
  1   0   0   0  1
 -1   1   0   0  1
 -1  -1   1   0  1
 -1  -1  -1   1  1
 -1  -1  -1  -1  1</pre>

<p>We have</p><pre class="sourceCode julia">L,U,p = lu(A)
U</pre>
<pre class="output">
5×5 Array{Float64,2}:
 1.0  0.0  0.0  0.0   1.0
 0.0  1.0  0.0  0.0   2.0
 0.0  0.0  1.0  0.0   4.0
 0.0  0.0  0.0  1.0   8.0
 0.0  0.0  0.0  0.0  16.0</pre>

<p>The bottom right entry gets big. If fact, for $n=20$ we have:</p><pre class="sourceCode julia">n = 20
A = I-tril(ones(Int, n,n),-1)
A[:,n] = ones(Int, n)
L,U,p = lu(A)
U[n,n]</pre>
<pre class="output">
524288.0</pre>

<p>This example is contrived as it produces the worst case results. Here we define:</p>$$~
g_n(A) = \frac{\max_{ij}|U_{ij}|}{\max_{ij}|A_{ij}|}.
~$$
<p>(The book defines a  related, but not identical quantity, but this fits more inline with the bounds given in the theorems.)</p><p>Then the worst case is $g_n(A) \leq 2^{n-1}$.</p><p>In theory then the bound of the form:</p>$$~
|\Delta_{ij}| \leq \frac{6}{5}(n+1) \epsilon |u_{ij}|
~$$
<p>Is just saying that the terms are like $n2^n\epsilon$, which for even modest size $n$ would be  a problem.</p><p>However, in practice the growth is closer to $n$ – not $2^n$ which means that the LU decomposition is assumed to be stable to use.</p><h3>Example</h3><p>Let's see the difference between a random $A$ and the one with the devious pattern:</p><pre class="sourceCode julia">n = 50
x = rand(n)

A = I-tril(ones(n,n),-1)
A[:,n] = ones(n)

L,U,p = lu(A)

b = A[p,:]*x
xtilde = U \ (L \ b)
norm(x - xtilde, Inf)/norm(x, Inf)</pre>
<pre class="output">
0.01676620745275773</pre>

<p>Whereas,</p><pre class="sourceCode julia">A = rand(n,n)
L,U,p = lu(A)

b = A[p,:]*x
xtilde = U \ (L \ b)
norm(x - xtilde, Inf)/norm(x, Inf)</pre>
<pre class="output">
2.5491915506233076e-13</pre>

<p>This is quite a difference. In the book, we find this bound:</p>$$~
\frac{\| x - \tilde{x}\|_\infty}{\|x\|_\infty} \leq 4n^2 g_n(A) \kappa_\infty(A) \epsilon
~$$
<p>The $\kappa$ is the condition number. This is for a slightly different definition of the growth number, but using ours we have:</p><pre class="sourceCode julia">growth(A,U=lu(A)[2]) = maximum(abs.(U)) / maximum(abs.(A))
bound(A) = 4*size(A)[1] * growth(A) * cond(A, Inf) * eps()</pre>
<pre class="output">
bound (generic function with 1 method)</pre>

<p>Then we have for our random matrix</p><pre class="sourceCode julia">bound(A)</pre>
<pre class="output">
6.849075494946706e-10</pre>

<p>This is a small bound and we see that the actual value is orders of magnitude less</p><p>And for our devious one – where there is a a big relative error – we have a very large bound:</p><pre class="sourceCode julia">A = I-tril(ones(n,n),-1)
A[:,n] = ones(n)
bound(A)</pre>
<pre class="output">
1250.0</pre>


  </div>
</div>  

</body>
</html>
