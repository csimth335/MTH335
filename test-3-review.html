
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"> 
    <meta name="viewport" content="width=device-width, initial-scale=1">




<link
  href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css"
  rel="stylesheet">

<style>
.julia {font-family: "Source Code Pro";
        color:#0033CC;
        }
body { padding-top: 60px; }
h5:before {content:"\2746\ ";}
h6:before {content:"\2742\ ";}
pre {display: block;}
</style>

<script src="http://code.jquery.com/jquery.js"></script>
<script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>

<!-- .julia:before {content: "julia> "} -->

<style></style>



<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ["\$","\$"], ["\\(","\\)"]]
  },
  displayAlign: "left",
  displayIndent: "5%"
});
</script>

<script type="text/javascript">
$( document ).ready(function() {
  $("h1").each(function(index) { 
       var title = $( this ).text()
       $("#page_title").html("<strong>" + title + "</strong>");
       document.title = title
  });
  $( "h2" ).each(function( index ) {
    var nm =  $( this ).text();                                    
    var id = $.trim(nm).replace(/ /g,'');
    this.id = id
    $("#page_dropdown").append("<li><a href='#" + id + "'>" + nm + "</a></li>");
  });
  $('[data-toggle="popover"]').popover();  
});
</script>

</head>


<body data-spy="scroll" >

<nav class="navbar navbar-default  navbar-fixed-top">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
         
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li><a href="#" id="page_title"></a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
         <li class="dropdown">
           <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
           Jump to... <span class="caret"></span></a>
          <ul class="dropdown-menu" role="menu" id="page_dropdown"></ul>
        </li>
      </ul>
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

<header>
</header>

<div class="container-fluid">
  <div class="span10 offset1">
<h1>Review for test 3</h1><p>Review questions for test 3. Some edits made 12/13. Test 3 will now only cover chapters 6-8</p><h2>Ch 6: Polynomial Interpolation</h2><p>The main goal: if we have points &#36;&#40;x_0, y_0&#41;, \dots, &#40;x_n, y_n&#41;&#36; where &#36;y_i &#61; f&#40;x_i&#41;&#36;, can we approximate the function &#36;f&#40;x&#41;&#36; with a polynomial &#36;p&#40;x&#41;&#36;?</p><blockquote>
<p>Thm: if the &#36;x_i&#36;'s are distinct, then there is a <em>unique</em> polynomial of degree &#36;n&#36; with &#36;p&#40;x_i&#41; &#61; y_i&#36;.</p>
</blockquote><p>There are two different ways to write this:</p><ul>
<li>Newton form: &#36;p_n&#40;x&#41; &#61; \sum_&#123;i&#61;0&#125;^n c_i \prod_&#123;j&#61;0&#125;^&#123;i-1&#125; &#40;x - x_j&#41;&#36;. We saw that the constants can be written in terms of divided differences: &#36;c_i &#61; f&#91;x_0, x_1, \dots, x_i&#93;&#36;.</li>
</ul><ul>
<li>Lagrange form: &#36;p_n&#40;x&#41; &#61; \sum y_k l_k&#40;x&#41;&#36;. We mentioned that the &#36;l_k&#36; can be written in a terms of a product:</li>
</ul>$$~
l_i(x) = \prod_{j=0, j \neq i}^n \frac{x - x_j}{x_i - x_j}.
~$$<p>The Newton form represents &#36;p&#36; in terms of polynomials of degree &#36;i&#36; whereas the Lagrange form has terms all of degree &#36;n&#36;.</p><h3>Error:</h3>$$~
f(x) - p_n(x) = \frac{1}{(n+1)!} f^{(n+1)}(\xi) \prod_{i=0}^n (x - x_i)
~$$<p>This gives a bound on the error.</p><h3>Convergence</h3><p>Even if it is true that for each &#36;x&#36;, &#36;f&#40;x&#41; - p_n&#40;x&#41; \rightarrow 0&#36; it need not be the case that &#36;\|f - p_n\|_\infty &#61; max_&#123;a \leq x \leq b&#125;|f&#40;x&#41; - p_n&#40;x&#41;| \rightarrow 0&#36;. As we saw on the computer there can be wild oscillations near the edges in &#36;p_n&#36;. In fact: if the interpolation points are evenly spaced the interpolating polynomials become unbounded.</p><h3>HW Problems</h3><p>6.1: 1, 2, 12</p><p>6.2: 4, 12, 13 (n=1 only)</p><h2>Ch7: Numeric differentiation and Integration</h2><p>Again, we have a table of function values &#36;&#40;x_0, y_0&#41;, \dots, &#40;x_n, y_n&#41;&#36; where &#36;y_i &#61; f&#40;x_i&#41;&#36;, What can we say about the derivate of &#36;f&#36;? The integral of &#36;f&#36;?</p><p>We had a basic scheme:</p><ul>
<li>Find &#36;p_n&#40;x&#41;&#36; an interpolating polynomial</li>
<li>Find &#36;p_n&#39;&#40;x&#41;&#36; of &#36;\int_a^b p_n&#40;x&#41;dx&#36;.</li>
<li>Approximate the desired answer by the computed answer.</li>
<li>Assess the error</li>
</ul><h3>Differentiation</h3><p>For differentiation it was mentioned that if we applied the above to points &#36;x&#36;, &#36;x-h&#36; and &#36;x_h&#36;, we would get the central difference formula:</p>$$~
f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}.
~$$<p>The forward difference equation is &#36;&#40;f&#40;x&#43;h&#41; - f&#40;x&#41;&#41;/h&#36;.</p><p>The error in the approximation – when done on the computer – has two sources of error:</p><ul>
<li>truncation error – what happens when using an approximate polynomial of degree &#36;n&#36; and not an infinite series</li>
<li>floating point error – resulting from putting the problem on the computer</li>
</ul><p>For the forward difference, the truncation error is <em>roughly</em> &#36;\mathcal&#123;O&#125;&#40;h&#41;&#36;, whereas the floating point error is &#36;\mathcal&#123;O&#125;&#40;\delta/h&#41;&#36;. So there needs to be a balance if using: take &#36;h&#36; small enough so the truncation error isn't large but not <em>too</em> small so that the floating point error isn't large.</p><p>The central difference has similar floating point error, but truncation error like &#36;\mathcal&#123;O&#125;&#40;h^2&#41;&#36;.  (It is basically &#36;f&#39;&#39;&#39;&#40;\xi&#41;/6h^2&#36;.)</p><p>We mentioned automatic differentiation, but this won't be on the test.</p><h2>Integration</h2><p>The process of approximating &#36;f&#36; by &#36;p&#36; and integrating &#36;p&#36; leads to 3 familar concepts:</p><ul>
<li>The Riemann sum is when &#36;n&#61;0&#36;, or using a constant for interpolation</li>
<li>The trapezoid rule is when &#36;n&#61;1&#36;, or using a linear polynomial for interpolation</li>
<li>Simpson't rule is when &#36;n&#61;2&#36;, that is a polynomial is used for interpolation</li>
</ul><p>Rather than globally approximate &#36;&#91;a,b&#93;&#36; with just a few points, what is done if that interval is partitioned and on each subpartition the approximation is used. With this we saw errors, &#36;\int_a^b f&#40;x&#41; dx - \int_a^b p_n&#40;x&#41; dx&#36;, given by</p><ul>
<li>Riemann &#36;&#40;b-a&#41;^2/n&#41;&#36;</li>
<li>Trapezoid &#36;-1/12f&#39;&#39;&#40;\xi&#41;&#40;b-a&#41;^3/n^2&#36;</li>
<li>Simpsons &#36;-1/180 f&#39;&#39;&#39;&#39;&#40;\xi&#41;&#40;b-a&#41;^5/n^4&#36;</li>
</ul><h3>Generalized</h3><p>The expression &#36;\int_a^b p_n&#40;x&#41;dx&#36; can be written differently taking LaGrange's form for polynomial interpolation:</p>$$~
\int_a^b f(x) dx \approx
\int_a^b p_n(x) dx - \int_a^b \sum_{k=0}^n f(x_k) l_k(x) dx
= \sum_{k=0}^n f(x_k) \int_a^b l_k(x)dx = \sum_{k=0}^n f(x_k) A_k.
~$$<p>The points &#36;x_k&#36; are called the <em>nodes</em> and the terms &#36;A_k&#36; the <em>weights</em>. Both can be precomputed, as they do not depend on &#36;f&#36;.</p><p>The main use here are quadrature formulas:</p><blockquote>
<p>(p493) Let &#36;w&#36; be a positive weight function (like &#36;w&#40;x&#41; &#61; 1&#36;) and let &#36;q&#36; be a non-zero polynomial of degree &#36;n&#43;1&#36; that is &#36;w&#36;-orthogonal to the space of polynomials of degree &#36;n&#36; or less. Then If &#36;x_0, x_1, \dots, x_n&#36; are the zeros of &#36;q&#36;, the quadrature formula derived by using these zeros as the nodes and used in the weight computation will be <em>exact</em> for any polynomial of degree &#36;2n&#43;1&#36; or less.</p>
</blockquote><p>We saw one family of orthogonal polynomials, those when &#36;w&#61;1&#36;. These were the Legendre polynomials satisfying the recursion:</p>$$~
P_0(x) = 1, \quad P_1(x) =x, \quad (n+1)P_{n+1}(x) = (2n + 1) x P_n(x)  - n P_{n-1}(x).
~$$<h3>Error</h3><p>If we defined</p>$$~
\int_a^b f(x) w(x) dx = \sum_{i=0}^n A_i f(x_i) + E
~$$<p>Then for &#36;f&#36; in &#36;C^&#123;2&#40;n&#43;1&#41;&#125;&#40;&#91;a,b&#93;&#41;&#36; the error can be written as:</p>$$~
E = \frac{1}{(2n)!} f^{(2(n+1))}(\xi) \int_a^b (\prod(x-x_i))^2 w(x) dx
~$$<h3>Questions</h3><p>7.1 6 (for &#36;f&#39;&#40;x&#41;&#36; only),</p><p>7.2: 1, 2, 27 (using 26 as given)</p><p>7.3: 3, 6</p><ul>
<li>The Gauss weights and nodes for &#36;n&#61;3&#36; are given by:</li>
</ul><pre class="sourceCode julia">nodes = [-sqrt(3 / 5), 0.0, sqrt(3 / 5)]
weights = [5 // 9, 8 // 9, 5 // 9]</pre>
<pre class="output">
3-element Array{Rational{Int64},1}:
 5//9
 8//9
 5//9</pre>

<p>Use these to estimate &#36;\int_&#123;-1&#125;^1 \sin&#40;x&#41; dx&#36;.</p><p>Use the error term and the fact that &#36;|f^&#123;&#40;2n&#41;&#125;&#40;\xi&#41; |&lt;1&#36; to estimate the error.</p><h2>Chapter 8. Differential Equations</h2><p>The IVP (initial value problem) is a specification about a function &#36;x&#40;t&#41;&#36; through a relation involving its derivative at time &#36;t&#36;:</p>$$~
x'(t) = f(t, x), \quad x(t_0) = x_0
~$$<p>In 8.1, we see that an IVP may not have an answer for all &#36;t&#36;; it may not have an answer at all; and if it does have an answer, it may not be unique.</p><p>There are theorems which will vouch for uniqueness and existence:</p><blockquote>
<p>Thm 1. Existence theorem</p>
</blockquote><p>If &#36;f&#36; is <em>continuous</em> on a rectangle centered at &#36;&#40;t_0, x_0&#41;&#36;, say:</p>$$~
R = \{ (t,x) : |t - t_0| \leq \alpha, |x - x_0| \leq \beta \}.
~$$<p>Then the inital value problem has a solution &#36;x&#40;t&#41;&#36; for &#36;|t - t_0| \leq min&#40;\alpha, \beta/M&#41;&#36; where &#36;M&#36; maximizes &#36;|f|&#36; in &#36;R&#36;.</p><blockquote>
<p>Thm 2 (p526). If both &#36;f&#36; and &#36;\partial f/\partial x&#36; are <em>continuous</em> in R, then there is a unique solution for &#36;|t - t_0| \leq min&#40;\alpha, \beta/M&#41;&#36;</p>
</blockquote><blockquote>
<p>Thm 3 If f is Lipshitz then the intial value problem will have a unique solution in some interval.</p>
</blockquote><p>Precisely, if &#36;f&#36; is continuous on the strip &#36;a \leq t\leq b&#36; and &#36;x \in &#40;-\infty, \infty&#41;&#36; <em>and</em> satisfies the inequality for a fixed &#36;L&#36;:</p>$$~
| f(t,x_1) - f(t, x_2) | \leq L | x_1 - x_2|
~$$<p>then the solution exists on the interval &#36;&#91;a,b&#93;&#36;.</p><hr /><p>The last two give conditions on &#36;f&#36; that guarantee an answer exists and is unique, at least for some values of &#36;t&#36;.</p><h3>Euler's method</h3><p>The granddaddy of all methods to numerically approximate the solution to an IVP is Euler's method:</p><p>From a sequence of time steps &#36;t_i &#61; t_0 &#43;  ih&#36;, where &#36;h&#36; is the small time step, Euler's method defines a sequence of &#36;x_i&#36; values through</p>$$~
x_{i+1} = x_{i} + f(t_i, x_i)
~$$<p>The approximate local truncation error at each step is like &#36;\mathcal&#123;O&#125;&#40;h^2&#41;&#36;, so if things are nice, the global truncation error will be like &#36;\mathcal&#123;O&#125;&#40;h&#41;&#36;, when the number of steps is basically &#36;1/h&#36;.</p><h3>Taylor methods</h3><p>The Euler method can be derived by starting with a Taylor series:</p>$$~
x(t + h) = x(t) + x'(t)h + x''(t)/2 \cdot h^2 + \cdots + x^{(n)}(t)/n! \cdot h^n + \mathcal{O}(h^{n+1})
~$$<p>Then truncating at the first order. As the IVP defines &#36;x&#39;&#40;t&#41;&#36; in terms of &#36;f&#36;, Euler's method is nothing more than the tangent line approximation.</p><p>Using more terms can be done by differentiating, though that gets tricky.</p><h3>Runge-Kutte methods</h3><p>The Runge - Kutte methods generalize Euler's method by adding various combinations of the approximations for &#36;f&#36;. The two discussed in class were:</p><ul>
<li>Heun's method where</li>
</ul>$$~
\begin{align}
x_{n+1} &= x_n + \frac{1}{2} F_1 + \frac{1}{2} F_2, \text{ where}\\
F_1(x) &= h\cdot f(t, x)\\
F_2(x) &= h\cdot f(t+h, F_1)
\end{align}
~$$<p>This has  local truncation errors of &#36;\mathcal&#123;O&#125;&#40;h^3&#41;&#36;</p><ul>
<li>The fourth order method</li>
</ul>$$~
\begin{align}
x_{n+1} &= x_n + \frac{1}{6}( F_1 + 4F_2 + 4F_3 + F_4), \text{ where}\\
F_1 &= h\cdot f(t_n, x_n)\\
F_2 &= h\cdot f(t_n+h, F_1/2)\\
F_3 &= h\cdot f(t_n+h, F_2/2)\\
F_2 &= h\cdot f(t_n+h, F_3)\\
\end{align}
~$$<p>This has  local truncation errors of &#36;\mathcal&#123;O&#125;&#40;h^5&#41;&#36;.</p><h3>Multistep methods</h3><p>The general multistep model might look like this (from p557)</p>$$~
a_k x_n + a_{k-1}x_{n-1} + \cdots + a_0 x_{n-k} =
h( b_k f_n + f_{k-1} f_{n-1} + \cdots + b_0 f_{n-k}).
~$$<p>We discussed:</p><ul>
<li>if &#36;b_k \neq 0&#36; the method is <em>implicit</em> otherwise it is explicit</li>
<li>Definining &#36;p&#40;x&#41;&#36; and &#36;q&#40;x&#41;&#36; using the coefficients &#36;a&#36; and &#36;b&#36; that the method converges only if all the roots of &#36;p&#36; are in or on the unit disc, and if on, they are simple roots <em>and</em> &#36;p&#40;1&#41;&#61;0&#36; and &#36;p&#39;&#40;1&#41; &#61; q&#40;1&#41;&#36;. (Consistent and stable)</li>
<li>If &#36;f_x &lt; \lambda&#36;, the if the local truncation error is &#36;\mathcal&#123;O&#125;&#40;h^&#123;m&#43;1&#125;&#41;&#36; then the global error is &#36;\mathcal&#123;O&#125;&#40;h^m&#41;&#36;</li>
</ul><p>Some special cases:</p><ul>
<li>Euler's method</li>
<li>Backward Euler</li>
<li>Adams Bashworth</li>
<li>Adams Moulton</li>
</ul><h3>Problems</h3><ul>
<li>Let an IVP be given by &#36;x&#39;&#40;t&#41; &#61; atan&#40;x&#41;, \quad x&#40;0&#41; &#61; 0&#36;. Is &#36;f&#40;t,x&#41;&#36;</li>
</ul><ul>
<li>continuous in the strip &#36;0 \leq t \leq 10&#36; and &#36;-\infty &lt; x &lt; \infty&#36;?</li>
<li>does it satisfy &#36;~ |f&#40;t,x_1&#41; - f&#40;t,x_2&#41;| \leq L |x_1 - x_2| ~&#36;</li>
</ul><ul>
<li>Use one of the theorems in section 8.1 to show p9 on page 528. You need to show that &#36;\beta/M&#36; can be as large of the value, where &#36;M&#36; is the biggest that &#36;f&#40;t,x&#41;&#36; can be in when &#36;|x| &lt; \beta&#36;.</li>
</ul><ul>
<li>Using &#36;h&#61;1/3&#36; use Euler's method to solve the IVP at &#36;t&#61;1&#36; with</li>
</ul>$$~
x'(t) = 10 - 3x \cdot t\quad
x(0) = 1
~$$<ul>
<li>Using &#36;h&#61;1/3&#36; use the Heun's method to solve the following IVP at &#36;t&#61;1&#36;. </li>
</ul>$$~
x'(t) = 1 + x^2,\quad x(0)=0
~$$<ul>
<li>problem 2, page 546</li>
</ul><ul>
<li>The Runge Kutte method is often called <code>rk45</code>, as the global error is order 4 and the local error order 5. Assuming this, what should be the step size &#36;h&#36; so that the error at time &#36;1&#36; (starting at &#36;0&#36; is basically &#36;10^&#123;-5&#125;&#36;?</li>
</ul><ul>
<li>The second-order Adams-Bashworth method is</li>
</ul>$$~
x_{n+1} = x_n + h[(3/2) f_n - (1/2) f_{n-1}]
~$$<p>Take &#36;h&#61;1/4&#36;. Using Euler's method to find &#36;x_1&#36;, find &#36;x_4&#36; when</p>$$~
x'(t) = t + x,\quad
x'(0) = 1
~$$<ul>
<li>The implicit Euler (backward) is</li>
</ul>$$~
x_{n+1} = x_n + h f(t_{n+1}, x_{n+1}).
~$$<p>For the IVP</p>$$~
x'(t) = -10x, \quad x(0) =1
~$$<p>The implicitness can be solved directly, as &#36;f&#36; is linear. Using &#36;h&#61;1/3&#36;, find &#36;x_3&#36;.</p><ul>
<li>The multistep method is convergent if it is both stable and consistent. Both these may be checked by accompanying polynomials.</li>
</ul><p>Show the  second-order Adams-Bashworth method </p>$$~
x_{n+1} = x_n + h[(3/2) f_n - (1/2) f_{n-1}]
~$$<p>is convergent.</p><ul>
<li>The Runge Kutta methods involve a certain number of function evaluations per step. The following table shows the best possible local truncation error for a given number of function evaluations:</li>
</ul><pre class="sourceCode julia">Evals  order
2  2
3  3
4  4
5  4
6  5
7  6
8  6
9  7</pre>
<ul>
<li>Why might this suggest using an order 4 method?</li>
</ul><ul>
<li>Euler's method has 1 evaluation for order 2. Compare the sum of the 1-step errors in using 4 steps of Euler of size &#36;h/4&#36; with one step of a fourth order RK method with order 4. What values of &#36;h&#36; would make one more attractive than the other?</li>
</ul>
  </div>
</div>  

</body>
</html>
