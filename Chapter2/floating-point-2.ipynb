
{
  "cells": [
     {"cell_type":"markdown","source":"<h1>Floating point numbers</h1>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<h2>Rounding, again</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>As not every number is a machine number, numbers are rounded to machine numbers. There are variants of rounding methods. Some are:</p>","metadata":{}},
{"cell_type":"markdown","source":"<ul><li>round to nearest: find the closest machine number. If a tie round to the even number.</li><li>round to $0$: round down if positive, up if negative</li><li>round to $\\infty$ (or $-\\infty$): always round up (or down)</li></ul>","metadata":{}},
{"cell_type":"markdown","source":"<h3>For example,...</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6000000000000001"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["with_rounding(Float32, RoundUp) do \n       .1 + .2 + .3\nend"],"metadata":{},"execution_count":null},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["with_rounding(Float32, RoundDown) do \n       .1 + .2 + .3\nend"],"metadata":{},"execution_count":null},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.6000000000000001"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["with_rounding(Float32, RoundDown) do \n    -.1 - .2 - .3\nend"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>and</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.6"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["with_rounding(Float32, RoundToZero) do \n       -.1 - .2 - .3\nend"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<h3>A more realistic problem</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["(-8.526512829121202e-13,-7.213998287625145e-10,7.215135156002361e-10)"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["xs = rand(10^7) - 0.5\nys = big(xs)\nrup = with_rounding(() -> sum(xs), Float64, RoundUp)\nrdown = with_rounding(() -> sum(xs), Float64, RoundDown)\nex = convert(Float64, sum(ys))\n\nex - sum(xs), ex - rup, ex - rdown"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<h3>How big can the error be in rounding one number?</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>We've seen this answered: the significand can be off by at most $1/2$<code>ulp</code> $=1/2 \\cdot 2^{-p}$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>At most the <em>error</em> is $1/2 \\cdot 2^{-p}\\cdot 2^m$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Write $fl(x)$ to be the machine number that $x$ rounds to. Then</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ e = |x - fl(x)| \\leq 1/2 \\cdot 2^{-p}\\cdot  2^m $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The <em>relative error</em> of rounding $x$ is at most</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ |\\frac{x - fl(x)}{x}| = |\\frac{e}{x}| = \\frac{1/2 \\cdot 2^{-p}\\cdot 2^m}{q 2^m} \\leq 1/2 \\cdot 2^{-p} $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>(With just chopping there would be no $1/2$.)</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>If $\\delta$ is the relative error, then we have $fl(x) = x (1 + \\delta)$ and $|\\delta| \\leq 1/2\\cdot 2^{-p}$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Next closest number</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Suppose $x = 1.a_1a_2 \\cdot a_p \\cdot 2^m$ is a machine number with precision $p$. What is the relative size of the next largest number? This would be</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ x' = (1.a_1a_2 \\cdot a_p + 2^{-p}) \\cdot  2^m $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The absolute difference being $2^{m-p}$. So if $m$ is larger, the difference is larger – bigger gaps. The <em>relative difference</em> is basically a constant: $2^{-p}2^m/(q 2^m) \\leq 2^{-p}$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>1+</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>What is the next number after 1? This would be</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ 1+ = 1.0000 \\cdots 0000 1 \\cdot 2^0 $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The difference $1^+ - 1 = 2^{-52}$ or</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.220446049250313e-16"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["2.0 ^ (-52)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>As mentioned, this value is given by:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.220446049250313e-16"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["eps()"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Or <code>nextfloat(1.0) - 1.0</code></p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>Error analysis of arithmetic operations</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Rounding can mess with our \"inituitive\" ideas of how numbers work: Consider the familiar decimal case with $p=3$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>What is $10.1 - 9.93$?</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>In regular subtraction we align the decimal points</p>","metadata":{}},
{"cell_type":"markdown","source":"<pre>10.10\n09.93\n-----\n00.17\n</pre>","metadata":{}},
{"cell_type":"markdown","source":"<p>Consider a <em>primitive</em> computer where the digits are shifted to align the decimal points. Hence $9.93$ could become $0.99 \\cdot 10^{1}$, if chopped. So that subtraction becomes</p>","metadata":{}},
{"cell_type":"markdown","source":"<pre>10 *  1.01\n      0.99\n      ----- \n10 *  0.02\n</pre>","metadata":{}},
{"cell_type":"markdown","source":"<p>The difference between $.20$ and $.17$ is 3 units in the off in the last digit of precision. So rounding can have an adverse effect.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>How far off can subtraction with shifting and truncation be?</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Suppose we have  precision $p$ and binary ($\\beta=2$). Then the <em>relative</em> error can be as large as 1 = $\\beta-1$!</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Consider a small case: $1.00 \\cdot 2^0$ and $1.11 \\cdot 2^{-1}$. (These are adjacent). Then mathematically the difference is $0.001$, but if $1.11 2^{-1}$ is shifted (and chopped) to $0.11 \\cdot 2^0$ to match, then the difference is $0.01$. We have $|(0.001 - 0.01)/(0.001)| =  1$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p><strong>Historically</strong> this led to engineering using guard digits (mentioned in the book). The IEEE 754 standard is different – the values should be exactly subtracted then rounded (exact rounding).</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>Analysis of floating point operations</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Consider more generally the basic operations of addition, subtraction, multiplication, and division.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Let's assume (contrary to above) that the operations on floating point are correctly done and <em>then</em> rounded to a machine number. (This can be arranged by using more bits for intermediate computations).</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>If $\\odot$ is any of the above operations, what is $fl(x \\odot y)$?</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>We know for $x$ that $fl(x) = x(1 + \\delta)$ where $\\delta$ is small ($\\leq 2^{-p}$) and depends on $x$. So,</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ fl(x \\odot y) = fl(fl(x) \\odot fl(y)) = ((x(1+\\delta)) \\odot (y(1 + \\delta)))(1 + \\delta) $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Each $\\delta$ is small and possibly different.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Well, how much off are we? Let's quickly check:</p>","metadata":{}},
{"outputs":[],"cell_type":"code","source":["using SymPy"],"metadata":{},"execution_count":null},
{"outputs":[{"output_type":"execute_result","data":{"text/latex":["$$d_{1} d_{2} d_{3} + d_{1} d_{2} + d_{1} d_{3} + d_{1} + d_{2} d_{3} + d_{2} + d_{3}$$"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["x,y,d1,d2,d3 = symbols(\"x,y,d1,d2,d3\", real=true)\nop = *\n( op(x*(1+d1), y*(1+d2)) * (1 + d3) - op(x,y))/op(x,y) |> expand"],"metadata":{},"execution_count":null},
{"outputs":[{"output_type":"execute_result","data":{"text/latex":["$$\\frac{d_{1} d_{3} y}{d_{2} y + y} + \\frac{d_{1} y}{d_{2} y + y} + \\frac{d_{3} y}{d_{2} y + y} + \\frac{y}{d_{2} y + y} - 1$$"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["op = /\n( op(x*(1+d1), y*(1+d2)) * (1 + d3) - op(x,y))/op(x,y) |> expand"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>But...</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/latex":["$$\\frac{d_{1} d_{3} x}{x + y} + \\frac{d_{1} x}{x + y} + \\frac{d_{2} d_{3} y}{x + y} + \\frac{d_{2} y}{x + y} + \\frac{d_{3} x}{x + y} + \\frac{d_{3} y}{x + y}$$"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["op = +\n( op(x*(1+d1), y*(1+d2)) * (1 + d3) - op(x,y))/op(x,y) |> expand"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<h2>A Leaky abstraction</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Floating point is a <a href=\"http://www.johndcook.com/blog/2009/04/06/numbers-are-a-leaky-abstraction/\">leaky</a> abstraction for the real numbers. Certain mathematical facts aren't true for floating point operations!</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Here are some \"gotchas\"</p>","metadata":{}},
{"cell_type":"markdown","source":"<p><li> subtraction of like sized values can be a problem</p>","metadata":{}},
{"cell_type":"markdown","source":"<p><li> there is no guarantee of <em>associativity</em> ($a+(b+c) = (a+b) + c$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p><li> there is no guarantee of <em>commutivity</em> ($a+b = b + a$)</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Subtraction</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Consider again the case of subtracting two numbers that are close by.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>We saw</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ fl(fl(x) - fl(y)) - (x-y) = \\frac{1}{x-y}(x (\\delta_1 + \\delta_3 + \\delta_1 \\delta_3) + y((\\delta_2 + \\delta_3 + \\delta_2 \\delta_3))). $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>If $x$ and $y$ are close, then this can be quite large.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>In the book, we have Theorem 1 of section 2.2</p>","metadata":{}},
{"cell_type":"markdown","source":"<blockquote><p>If $x$ and $y$ are binary floating point numbers with $x > y > 0$ with $$2^{-q} \\leq 1 - y/x \\leq 2^{-p}$$ Then at most $q$ and <em>at least</em> $p$ significant binary bits are lost in the substraction $x-y$. </p></blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Idea</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Suppose we  are in decimal and we have $4$ digits of precision. Consider subtracting $22/7 = 3.142857142857143$ from $\\pi = 3.1415926535897...$. We have</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ fl(\\pi) - fl(22/7) = 3.142 - 3.143 = -0.001 = -1.000 \\cdot 10^{-3}. $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The actual answer is $-0.0012644892...$ rounded to  $-1.264\\cdot 10^{-3}$. Where did the extra zeros come from in the answer above? They are just added on as there is no obvious alternative when shifted to normalized scientific notation. So we lost 3 digits of accuracy and we have $10^{-3} \\leq 1 - \\pi/(22/7) \\leq 10^{-4}$. </p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Proof</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The lower bound:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Say $x = r \\cdot 2^n$ and $y=s\\cdot 2^m$ with $m \\leq n$ and here $1/2 \\leq r, s < 1$. Then to \"line up the decimal points\" we may write $y = s \\cdot 2^{m-n} \\cdot 2^n$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ x - y = (r - s\\cdot 2^{m-n}) \\cdot 2^n $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The significand then satisfies:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ r - s \\cdot 2^{m-n} = r(1 - \\frac{s\\cdot 2^m}{r \\cdot 2^n}) = r(1 - y/x) < 2^{-p} $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>To put into normalized floating point, the significand must be shifted (there are leading $0$s) and the (at least) $p$ terms added are spurious, so accuracy is lost.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Example</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Consider $\\sin(x) \\approx x$. So $\\sin(x) - x$ will cause issues.</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["-5.086014673919698e-6"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["x = 1/2^5\nX = big(1/2^5)   # more precision\nsin(x) - x"],"metadata":{},"execution_count":null},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["-5.086014673921260418878742535535373921881201068959602540274778455791996714357325e-06 with 256 bits of precision"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["sin(X) - X"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Only accurate to the 10th digit – not the 16th. There is a loss of accuracy</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Compare this to addition:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.062494913985326084"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["sin(x) + x"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>and</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.249491398532607873958112125746446462607811879893104039745972522154420800328564e-02 with 256 bits of precision"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["sin(X) + X"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>The moral of the story – try to avoid subtraction when the values are of the same size.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>(Yes, but what about $f(x+h) - f(x)$?)</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>floating point is not always associative</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1,1)"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["a,b,c = 10^30, -10^30, 1\na + (b + c),  (a + b) + c # not associative, even with machine numbers"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>And:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.6000000000000001,0.6)"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["(0.1 + 0.2) + 0.3,  0.1 + (0.2 + 0.3)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<h3>Floating point is not always commutative</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.6000000000000001,0.6)"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["0.1 + 0.2 + 0.3, 0.3 +  0.2 +  0.1"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Moral: need to be careful when trying to say two things are exactly equal.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Proof not associative</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Let $a$, $b$ and $c$ be machine numbers. Then we have $fl(a+b) = (a+b)(1+\\delta)$, where delta may be $0$, is small, but may not be $0$ and depends on $a$ and $b$. So:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ \\begin{align} fl((a+b)+c) &= ((a+b)(1+\\delta_1)+c)(1+\\delta_2) = (a+b)(1+\\delta_1)(1+\\delta_2) + c(1+\\delta_2)\\\\ fl((a+(b +c) &= (a+ (b+c)(1+\\delta_3))(1+\\delta_4) =  a(1+\\delta_4) + (b+c)(1+\\delta_3)(1+\\delta_4) \\end{align} $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Are these equal? Add smaller first?</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>finessing the problem</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Consider the problem of the quadratic equation and computing $b^2 - 4ac$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>An example of <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.6768&rep=rep1&type=pdf\">Goldberg</a> of Catastrophic cancellation using rounding to three significant digits illustrates. Suppose $a=1.22$, $b=3.34$ and $c=2.28$.  Then</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.029200000000001225"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["a,b,c = 1.22, 3.34, 2.28\nb^2 - 4a*c"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>But if we rounded $b^2$ and $4ac$ to 3 significant digits, we would have</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.09999999999999964"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["round(b^2,1) - round(4a*c, 1)  ## b^2 > 10, so round(_,1) is 3 digits, ..."],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>The subtraction isn't the issue here, it is that rounding of floating point numbers to machine numbers can be a source of loss of accuracy.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Case study: finding the midpoint of two numbers</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>In an example in the book (p76) a formula to find the midpoint – the point halfway between $a$ and $b$ – is given:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ a + \\frac{b-a}{2} $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>And not $(a+b)/2$. Why?</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Error in $(a+b)$ can be of size $(a+b)\\delta$ which can be large. The error in $b-a$ is generally smaller.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Their general strategem is:</p>","metadata":{}},
{"cell_type":"markdown","source":"<blockquote><p>in numerical calculations it is best to compute a quantity by adding a small correction term to a previous approximation. </p></blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Case study: Polynomial evaluation</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Polynomial evaluation can be sensitive to numeric issues. Problem 3 on page 62 considers a polynomial evaluated 3 ways:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ \\begin{align} h(x) &= (x-1)^8 \\\\ f(x) &= x^8 - 8x^7 + 28x^6 - 56x^5 + 70x^4 - 56x^3 + 28x^2 - 8x + 1 \\\\ g(x) &= (((((((x-8)x + 28)x - 56)x + 70)x -56)x + 28) - 8)x + 1 \\end{align} $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>They are interested in values near $1$ (101 values between 0.99 and 1.01 in fact). Let's see the differences</p>","metadata":{}},
{"outputs":[{"data":{"text/plain":["Plot(...)"],"image/png":"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"}}],"cell_type":"code","source":["using Gadfly\nh(x) = (x-1.0)^8\nf(x) = x^8 - 8x^7 + 28x^6 - 56x^5 + 70x^4 - 56x^3 + 28x^2 - 8x + 1\ng(x) = (((((((x-8)*x + 28)*x - 56)*x + 70)*x -56)*x + 28)*x - 8)*x + 1\nxs = linspace(0.99, 1.01, 101)\nhs = map(h, xs); fs = map(f, xs); gs = map(g, xs)\nplot(layer(x=xs, y=hs, Geom.line, Theme(default_color=color(\"red\"))),\nlayer(x=xs, y=fs, Geom.line, Theme(default_color=color(\"blue\"))),\nlayer(x=xs, y=gs, Geom.line,Theme(default_color=color(\"green\")))\n)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>The red one is the most exact, as we would expect. It is basically 8 operations.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The blue has <em>more</em> variability than the green. Why? More operations:</p>","metadata":{}},
{"cell_type":"markdown","source":"<ul><li>$g$ has only $7$ multiplications -- $f$ has $9 + 8 + 7 + 6 + 5 + \\dots + 2$.</li><li>$f$ has large numbers $x^8$ with small numbers $1$ being added. This can cause loss of precision.</li></ul>","metadata":{}},
{"cell_type":"markdown","source":"<p>Moral $g$ is \"better.\"</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The method for $g$ is called \"Horner's\" method.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>You may have seen it in synthetic division:</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Synthetic division</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Evaluate the polynomial $f(x) = x^4 - 4x^3 + 6x^2 - 4x + 3$ at $x=2$. (The answer is $3$):</p>","metadata":{}},
{"cell_type":"markdown","source":"<pre>2  |  1  -4  6  -4  3\n      .   2 -4   4  0 \n   ------------------\n      1  -2  2   0  3\n</pre>","metadata":{}},
{"cell_type":"markdown","source":"<p>Notice, in the above the numbers are not large in any sense unliek $16 - 32 + 24 -8 + 3$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The algorithm is on page 21: Writing $p(x) = a_nx^n + \\cdots a_2 x^2 + a_1 x + a_0$ we have:</p>","metadata":{}},
{"cell_type":"markdown","source":"<pre>s = an\nfor i = n-1 to 0\n   s = s*x + a_i\nend\n</pre>","metadata":{}},
{"cell_type":"markdown","source":"<p>The order is different, but in <code>Julia</code> there is a \"macro\" to do this:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["@evalpoly(2, 3, -4, 6, -4, 1)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<h3>Kahan summation</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Kahan <a href=\"https://en.wikipedia.org/wiki/Kahan_summation_algorithm\">summation</a> is a means to compensate for the error when sums are made. Here is the basic algorithm:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["ksum (generic function with 1 method)"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["function ksum(s, a, c)\n  y = a - c\n  t = s + y\n  c = (t-s) - y\n  s = t\n  s, c\nend"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>It <em>should</em> be that $t-s$ is $y$ and so $c=0$. But if $s$ is large and $y$ small, then there is a loss of precision. The value of $c$ adjusts for this:</p>","metadata":{}},
{"cell_type":"markdown","source":"<ul><li>$(t-s)$ gets the high bits of $y$, subtracting $y$ from this sets $c$ to the *minus* the low bits of $y$. These are then subtracted off in the next step.</li></ul>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Testing ...</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.0,0.0,5.466863512992859e-7)"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["xs = rand(10^7);\nS = sum(map(big, xs)) |> x -> convert(Float64, x) ## using 256 bits\nss = 0.0; for i in 1:length(xs) ss = ss + xs[i] end\ns = sum(xs)\nks, c = 0.0, 0.0\nwhile(length(xs) > 0)\n  ks,c = ksum(ks, shift!(xs), c)\nend\nS - ks, S-s, S - ss"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<h2>Addition of numbers and cumulative error</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Errors accumulate. How big can they get???</p>","metadata":{}},
{"cell_type":"markdown","source":"<blockquote><p>Theorem 1 (p49) relative error in $\\sum_0^n x_i$ is $(1 + \\epsilon)^n-1 \\approx n\\epsilon$. </p></blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<p>Let $S_{k+1} = x_{k+1} + S_k$ be the partial sum and $T_{k+1} = fl(x_{k+1} + T_k) = (x_{k+1}+ T_k)(1+\\delta)$ be the floating point partial sum. What is the relative difference?</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ \\begin{align} \\frac{S_{k+1} - T_{k+1}}{S_{k+1}} &= \\frac{S_{k+1}(1+\\delta) - T_{k+1} - S_{k+1}\\delta}{S_{k+1}}\\\\ &= \\frac{(S_k + x_{k+1})(1+\\delta) - (T_k + x_{k+1})(1+\\delta) - S_{k+1}\\delta}{S_{k+1}}\\\\ &= (1 + \\delta)\\frac{S_k - T_k}{S_k} \\cdot \\frac{S_k}{S_{k+1}} - \\delta \\end{align} $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Let $\\rho_k$ be the absolute value. Since $|\\delta| \\leq \\epsilon$, it follows that with $\\rho_0 = 0$:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ \\rho_{k+1} \\leq \\rho_k(1+\\epsilon) + \\epsilon. $$ </p>","metadata":{}},
{"cell_type":"markdown","source":"<p>This can be solved to yield: $\\rho_n \\leq (1 + \\epsilon)^n - 1$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Other bounds</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The maximal possible error for accumulating positive sums grows <em>linearly</em> with the number of sums.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>If on average the summands are equally likely to be negative as positive, then a square root of $n$ is the growth.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Better algorithms are possible. Kahan summation is $\\mathcal{O}(n\\epsilon^2)$, so until $n \\approx 1/\\epsilon$ the errors are not noticeable. Though this method is a bit slow.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>In Julia, <a href=\"https://en.wikipedia.org/wiki/Pairwise_summation\">pairwise</a> summation is used. This has relative error given by $\\epsilon \\log_2(n)$. For $n=10^7$, this is about $23 \\epsilon$, so around $5e-15$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>Accuracy of functions</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Numbers may be approximate, so after applying a function the error can propagate. How much? Suppose $x$ is the real value and $\\bar{x}$ is a machine approximation. Further, assume our function $f$ is perfectly defined and $C^1$. Then [why?]:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ f(x) - f(\\bar{x}) = f'(\\xi)(x - \\bar{x}). $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The relative error divides this difference by $f(x)$ to get:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ |\\frac{f(x) - f(\\bar{x})}{f(x)}| = |x|\\frac{f'(\\xi)}{f(x)} \\cdot \\frac{x - \\bar{x}}{x} $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>(Written to emphasize a factor times the relative error in the approximation to the number $x$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Redundant functions</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>In <code>Julia</code> there are many \"redundant\" functions:</p>","metadata":{}},
{"cell_type":"markdown","source":"<ul><li>`sinpi` for computing $\\sin(\\pi x)$, `cospi`, ...</li><li>`expm` to compute $e^x - 1$ for $x$ near 0</li><li>`log1p` to compute $\\log(1+x)$ near 0</li></ul>","metadata":{}},
{"cell_type":"markdown","source":"<p>The reason for <code>expm</code> seems clear. $e^x \\approx 1 + x + x^2/2! + \\cdot$, so $e^x-1$ for small $x$ is a subtraction of like-sized values.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>For <code>log1p</code> <a href=\"http://www.johndcook.com/blog/2010/06/07/math-library-functions-that-seem-unnecessary/\">Cook</a> if $x$ is really small, say $x=10^{-17}$ (<code>x < eps()</code>), then what happens to $1 + x$? In floating point it is 1. But $\\log(1+x) \\approx x$ by Taylor, so the absolute error is $x$ and the relative error $1$. Quite large. There are more floating point values closer to $0$ than $1$, so smaller values of $x$ can be used in <code>log1p</code>. </p>","metadata":{}},
{"cell_type":"markdown","source":"<p>In <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.6768&rep=rep1&type=pdf\">What Every ...</a> there is a theorem that this function</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ \\begin{cases} x & \\text{if } 1 \\oplus x = 1\\\\ \\frac{x \\log(1+x)}{(1+x) - 1}& \\text{ otherwise} \\end{cases} $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>has relative error less than $5\\epsilon$ when $0 \\leq x \\leq 3/4$ and ... (guard digit, $\\log$ computed to within 1/2 ulp).</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>What about <code>sinpi</code>? It is needed:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["(-3.9082928156687315e-8,0)"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["n = 10^8\nsin(pi*n), sinpi(n)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Why? The definition of sine involves a reduction to the range $[0,2pi]$. Let's just see:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5.0,0.0)"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["divrem(pi * 10, 2pi)"],"metadata":{},"execution_count":null},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4.9999999e7,6.283185280343126)"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["divrem(pi * 10^8, 2pi)  ## 2e-8 from 2pi"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Note though</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50000000,0)"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["divrem(10^8, 2)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<h3>Case Study: The Log function</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>From <code>openlibm</code></p>","metadata":{}},
{"cell_type":"markdown","source":"<pre>\n/* __ieee754_log(x)\n * Return the logrithm of x\n *\n * Method :                  \n *   1. Argument Reduction: find k and f such that \n *\t\t\tx = 2^k * (1+f), \n *\t   where  sqrt(2)/2 < 1+f < sqrt(2) .\n *\n *   2. Approximation of log(1+f).\n *\tLet s = f/(2+f) ; based on log(1+f) = log(1+s) - log(1-s)\n *\t\t = 2s + 2/3 s**3 + 2/5 s**5 + .....,\n *\t     \t = 2s + s*R\n *      We use a special Reme algorithm on [0,0.1716] to generate \n * \ta polynomial of degree 14 to approximate R The maximum error \n *\tof this polynomial approximation is bounded by 2**-58.45. In\n *\tother words,\n *\t\t        2      4      6      8      10      12      14\n *\t    R(z) ~ Lg1*s +Lg2*s +Lg3*s +Lg4*s +Lg5*s  +Lg6*s  +Lg7*s\n *  \t(the values of Lg1 to Lg7 are listed in the program)\n *\tand\n *\t    |      2          14          |     -58.45\n *\t    | Lg1*s +...+Lg7*s    -  R(z) | <= 2 \n *\t    |                             |\n *\tNote that 2s = f - s*f = f - hfsq + s*hfsq, where hfsq = f*f/2.\n *\tIn order to guarantee error in log below 1ulp, we compute log\n *\tby\n *\t\tlog(1+f) = f - s*(f - R)\t(if f is not too large)\n *\t\tlog(1+f) = f - (hfsq - s*(hfsq+R)).\t(better accuracy)\n *\t\n *\t3. Finally,  log(x) = k*ln2 + log(1+f).  \n *\t\t\t    = k*ln2_hi+(f-(hfsq-(s*(hfsq+R)+k*ln2_lo)))\n *\t   Here ln2 is split into two floating point number: \n *\t\t\tln2_hi + ln2_lo,\n *\t   where n*ln2_hi is always exact for |n| < 2000.\n *\n * Special cases:\n *\tlog(x) is NaN with signal if x < 0 (including -INF) ; \n *\tlog(+INF) is +INF; log(0) is -INF with signal;\n *\tlog(NaN) is that NaN with no signal.\n *\n * Accuracy:\n *\taccording to an error analysis, the error is always less than\n *\t1 ulp (unit in the last place).\n *\n * Constants:\n * The hexadecimal values are the intended ones for the following \n * constants. The decimal values may be used, provided that the \n * compiler will convert from decimal to binary accurately enough \n * to produce the hexadecimal values shown.\n */\n\n#include \"openlibm.h\"\n#include \"math_private.h\"\n\nstatic const double\nln2_hi  =  6.93147180369123816490e-01,\t/* 3fe62e42 fee00000 */\nln2_lo  =  1.90821492927058770002e-10,\t/* 3dea39ef 35793c76 */\ntwo54   =  1.80143985094819840000e+16,  /* 43500000 00000000 */\nLg1 = 6.666666666666735130e-01,  /* 3FE55555 55555593 */\nLg2 = 3.999999999940941908e-01,  /* 3FD99999 9997FA04 */\nLg3 = 2.857142874366239149e-01,  /* 3FD24924 94229359 */\nLg4 = 2.222219843214978396e-01,  /* 3FCC71C5 1D8E78AF */\nLg5 = 1.818357216161805012e-01,  /* 3FC74664 96CB03DE */\nLg6 = 1.531383769920937332e-01,  /* 3FC39A09 D078C69F */\nLg7 = 1.479819860511658591e-01;  /* 3FC2F112 DF3E5244 */\n\n...\n</pre>","metadata":{}},
{"cell_type":"markdown","source":"<h2>2.3 Stable and unstable computations</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The book gives an example of an iterative sequence:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ \\begin{cases} x_0 = 1, x_1=1/3&\\\\ x_{n+1} = 13/3 x_n - 4/3 x_{n-1} & (n \\geq 1) \\end{cases} $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>What happens as $n$ gets larger:</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Using exact fractions</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["17-element Array{Float64,1}:\n 1.0        \n 0.333333   \n 0.111111   \n 0.037037   \n 0.0123457  \n 0.00411523 \n 0.00137174 \n 0.000457247\n 0.000152416\n 5.08053e-5 \n 1.69351e-5 \n 5.64503e-6 \n 1.88168e-6 \n 6.27225e-7 \n 2.09075e-7 \n 6.96917e-8 \n 2.32306e-8 "]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["xs = [1//1, 1//3]\nfor i in 1:15\n   xn_1, xn = xs[end-1:end]\n   push!(xs, 13//3 * xn - 4//3 * xn_1)\nend\nxs = map(float, xs)   "],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>(Or you could solve $x_2 = 13/9 - 4/3 = 1/9$, $x_3 = 13/27 - 4/9 = 1/27$, ... $x_n=(1/3)^n$.)</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Float32</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Using <code>Float32</code> like the book:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["17x2 Array{Any,2}:\n 1.0           1.0        \n 0.333333      0.333333   \n 0.111111      0.111111   \n 0.037037      0.0370373  \n 0.0123457     0.0123466  \n 0.00411523    0.00411871 \n 0.00137174    0.00138569 \n 0.000457247   0.000513056\n 0.000152416   0.000375651\n 5.08053e-5    0.000943748\n 1.69351e-5    0.00358871 \n 5.64503e-6    0.0142927  \n 1.88168e-6    0.0571502  \n 6.27225e-7    0.228594   \n 2.09075e-7    0.914374   \n 6.96917e-8    3.65749    \n 2.32306e-8   14.63       "]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["ys = [1.0, 1/3]\nys = [convert(Float32, y) for y in ys]\nfor i in 1:15\n   yn_1, yn = ys[end-1:end]\n   push!(ys, 13//3 * yn - 4//3 * yn_1)\nend\n[xs ys]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<h3>Relative errors are way off:</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["17-element Array{Any,1}:\n       0.0        \n      -2.98023e-8 \n      -4.76837e-7 \n      -5.84126e-6 \n      -7.06166e-5 \n      -0.000847619\n      -0.0101712  \n      -0.122054   \n      -1.46465    \n     -17.5758     \n    -210.909      \n   -2530.91       \n  -30371.0        \n -364452.0        \n      -4.37342e6  \n      -5.2481e7   \n      -6.29772e8  "]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["[(x-y)/x for (x,y) in zip(xs, ys)]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<h3>Float64</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Using 64-bit floating point</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["17x3 Array{Any,2}:\n 1.0           1.0           1.0        \n 0.333333      0.333333      0.333333   \n 0.111111      0.111111      0.111111   \n 0.037037      0.0370373     0.037037   \n 0.0123457     0.0123466     0.0123457  \n 0.00411523    0.00411871    0.00411523 \n 0.00137174    0.00138569    0.00137174 \n 0.000457247   0.000513056   0.000457247\n 0.000152416   0.000375651   0.000152416\n 5.08053e-5    0.000943748   5.08053e-5 \n 1.69351e-5    0.00358871    1.69351e-5 \n 5.64503e-6    0.0142927     5.64498e-6 \n 1.88168e-6    0.0571502     1.88147e-6 \n 6.27225e-7    0.228594      6.26395e-7 \n 2.09075e-7    0.914374      2.05752e-7 \n 6.96917e-8    3.65749       5.63989e-8 \n 2.32306e-8   14.63         -2.99408e-8 "]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["zs = [1.0, 1/3]\nfor i in 1:15\n   zn_1, zn = zs[end-1:end]\n   push!(zs, 13//3 * zn - 4//3 * zn_1)\nend\n[xs ys zs]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<h3>What happens?</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Any error in $x_n$ is multiplied by $13/3$. So in particular, any error in $x_1$ is propagated to $x_n$ with a possible error of $(13/3)^n \\delta$. So even is $\\delta$ is small the possible error is big (and relative error even bigger):</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["425.24729871286013"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["(13/3)^15 * eps(Float32)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>The same happens with 64 bits, just slower as the bounds on $\\delta$ are smaller.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>### What about different starting values?</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Had the problem been started differently, say $x_0=1$, $x_1=4$. Then the error in $x_1$ is still propagated, but the relative error stays small, as the sequence grows as $4^n$ and does not decay like $(1/3)^n$:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["32-element Array{Any,1}:\n 0.0     \n 0.916667\n 0.993056\n 0.999421\n 0.999952\n 0.999996\n 1.0     \n 1.0     \n 1.0     \n 1.0     \n ⋮       \n 1.0     \n 1.0     \n 1.0     \n 1.0     \n 1.0     \n 1.0     \n 1.0     \n 1.0     \n 1.0     "]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["ys = [1, 4+eps()]\nys = [convert(Float32, x) for x in xs]\nfor i in 1:15\n   xn_1, xn = xs[end-1:end]\n   yn_1, yn = ys[end-1:end]\n   push!(xs, 13//3 * xn - 4//3 * xn_1)\n   push!(ys, 13/3 * yn - 4//3 * yn_1)\nend\nxs = [4^(n-1) for n in 1:length(ys)]   \n[(x-y)/x for (x,y) in zip(xs, ys)]"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>Yet the \"error\" is large.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Case study: different approaches to the same problem lead to different answers...</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Consider the case of computing $I_n = \\int_0^1 x^n e^{-x}$. (Attributed to Forsythe, Malcom and Moler 1977). Using integration by parts gives:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ I_n = n I_{n-1} - 1/e $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Or reversing $I_{n-1} = 1/n (I_n + 1/e)$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>forward solving</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Starting with $I_0 = e - 1$ we can get $I_n$:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.303461008663297e64"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["S = e - 1\nfor n in 1:50\n  S = n * S - 1/e\nend\nS"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>All good, right?</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Well, ...</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.007354706795800189"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["using SymPy\nn = 50\nx = symbols(\"x\")\nintegrate(x^n *exp(-x), (x, 0, 1)) |> N"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>The limit is ...</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>### What happens</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>From the expression $n I_{n-1}$, any error is multiplied by $n$. So If $I_1$ is off by $\\epsilon$, then $I_2$ is off by $2\\epsilon$$, so $I_3$ is off by $3\\cdot 2 \\epsilon$, ... and $I_50$ is off by $50! \\epsilon$. which is <code>3e64</code>.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Working backwards isn't the same issue as we have $1/n \\cdot I_n$ so errors are multiplied by $1/n$ not $n$. So we would just need a starting point:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.007354706795800189,0.007354706795800189)"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["n = 60\nS = integrate(x^n *exp(-x), (x, 0, 1)) |> N\nfor n in 60:-1:51\n  S = 1/n * (S + 1/e)\nend\nS, integrate(x^50 *exp(-x), (x, 0, 1)) |> N"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<h2>Conditioning</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Some algorithms perform well under certain inputs and poorly under others. When can you tell?</p>","metadata":{}},
{"cell_type":"markdown","source":"<blockquote><p>Condition: how sensitive is a solution to small changes in the input? ill-conditioned: small changes can produce large changes. condition number: for some problems a numeric value can be assigned. </p></blockquote>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Function evaluation</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Suppose our \"algorithm\" is just $x \\rightarrow f(x)$.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>What is $f(x+h)$ for small $h$ <em>compared</em> with $f(x)$?</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ \\text{error} = f(x+h) - f(x) = f'(\\xi) h, \\quad \\xi \\text{ between $x$ and $x+h$}. $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The <em>relative error</em> is then</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ \\begin{align} \\text{relative error} &= \\frac{f(x+h) - f(x)}{f(x)}\\\\ &= \\frac{f'(\\xi)h}{f(x)} \\approx \\frac{xf'(x)}{f(x)} \\frac{h}{x}. \\end{align} $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Example.</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>What is the condition number when evaluating $\\sqrt(x)$ near $1/2$?</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ \\frac{x f'(x)}{f(x)} = \\frac{x \\cdot 1/\\sqrt{x}}{\\sqrt{x}} = 1 $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>So small.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Evaluation of a root with *uncertainty* in the function</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Think about a polynomial with floating point coefficients – there is rounding.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Let $f(x)$ have a root $r$ and suppose $F(x) = f(x) + \\epsilon g(x)$. Then suppose $F$ has a root $r+h$ (nearby, but effected by $\\epsilon$. What is $h$? Assume both $f$ and $g$ have Taylor series so that:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ 0 = F(r +h) = f(r) + f'(r)h + \\mathcal(O)(h^2) + \\epsilon \\left( g(r) + g'(r)h + \\mathcal(O)(h^2) \\right) $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Dropping the big $\\mathcal{O}$ term and using $f(r)=0$, gives:</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ 0 \\approx f'(r)h + \\epsilon g(r)  + \\epsilon g'(r)g $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Or</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ h \\approx -\\epsilon \\frac{g(r)}{f'(r) + \\epsilon g'(r)} \\approx -\\epsilon \\frac{g(r)}{f'(r)}. $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Wilkinson example</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The Wilkinson polynomial is $f(x) = (x-1)(x-2) \\cdots (x-20)$. Suppose, the coefficients are perturbed by $10^{-4}x^{20}$. ($\\epsilon = 10^{-4}$ and $g(x) = x^{20}$. </p>","metadata":{}},
{"cell_type":"markdown","source":"<p>We have $g(20) = 20^{20}$. What is $f'(20)$? It is $(20-1)(20-2) \\cdots (20-19) = 19!$. (Why?) And so the condition number is</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>$$ -\\epsilon \\frac{20^{20}}{19!} $$</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>With $\\epsilon=10^{-4}$ this is:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["86199.60824364352"]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["1e-4 * 20.0^20 / factorial(19)"],"metadata":{},"execution_count":null},
{"cell_type":"markdown","source":"<p>That is almost $10^5$! So a small perturbation in this problem <em>could</em> lead to dramatically different roots.</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["20-element Array{Complex{Float64},1}:\n     1.0+0.0im     \n     2.0+0.0im     \n     3.0+0.0im     \n     4.0+0.0im     \n     5.0+0.0im     \n 6.00005+0.0im     \n 6.99956+0.0im     \n 8.00288+0.0im     \n 8.98674+0.0im     \n 10.0499+0.0im     \n 10.8862+0.0im     \n 12.3578+0.0im     \n  12.562+0.0im     \n 14.5189+0.213088im\n 14.5189-0.213088im\n 16.2068+0.0im     \n 16.8857+0.0im     \n 18.0301+0.0im     \n 18.9939+0.0im     \n 20.0005+0.0im     "]},"metadata":{},"execution_count":null}],"cell_type":"code","source":["using Roots\np(x) = prod([x-i for i in 1:20])\nroots(p) "],"metadata":{},"execution_count":null}
    ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.9",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0

}
